<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Cheng Zhang</title>
    <link>https://chengzhag.github.io/publication/</link>
      <atom:link href="https://chengzhag.github.io/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 22 Dec 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengzhag.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Publications</title>
      <link>https://chengzhag.github.io/publication/</link>
    </image>
    
    <item>
      <title>Unified Camera Positional Encoding for Controlled Video Generation</title>
      <link>https://chengzhag.github.io/publication/ucpe/</link>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/ucpe/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headercvpr-2026div&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;CVPR 2026&lt;/div&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://leeby68.github.io&#34; target=&#34;_blank&#34;&gt;Boying Li&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/Meng-Wei&#34; target=&#34;_blank&#34;&gt;Meng Wei&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://yanpei.me&#34; target=&#34;_blank&#34;&gt;Yan-Pei Cao&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://dinhphung.ml&#34; target=&#34;_blank&#34;&gt;Dinh Phung&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC&lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://github.com/VAST-AI-Research&#34; target=&#34;_blank&#34;&gt;VAST&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;!-- &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://github.com/chengzhag/UCPE&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;a href=&#34;https://www.youtube.com/watch?v=DogzWyoVBEs&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Video
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2512.07237&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2512.07237&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/DogzWyoVBEs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Our UCPE introduces a geometry-consistent alternative to PlÃ¼cker rays as one of the core contributions, enabling better generalization in Transformers. We hope to inspire future research on camera-aware architectures.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TLDR&lt;/h2&gt;
&lt;p&gt;ðŸ”¥ Camera-controlled text-to-video generation, now with intrinsics, distortion and orientation control!&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;cameras.png&#34;
       height=&#34;120&#34;
       style=&#34;display:inline-block; margin-right:48px;&#34;&gt;
  &lt;img src=&#34;orientation.png&#34;
       height=&#34;140&#34;
       style=&#34;display:inline-block;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;ðŸ“· UCPE integrates Relative Ray Encodingâ€”which delivers significantly better generalization than PlÃ¼cker across diverse camera motion, intrinsics and lens distortionsâ€”with Absolute Orientation Encoding for controllable pitch and roll, enabling a unified camera representation for Transformers and state-of-the-art camera-controlled video generation with just 0.5% extra parameters (35.5M over the 7.3B parameters of the base model)&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;video-ucpe.gif&#34;
       alt=&#34;UCPE&#34;
       style=&#34;max-height:480px; width:auto;&#34;&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;
&lt;p&gt;Our &lt;strong&gt;Relative Ray Encoding&lt;/strong&gt; not only generalizes to but also enables controllability over a wide range of camera intrinsics and lens distortions.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;video-lens.gif&#34;
       alt=&#34;Lens control&#34;
       style=&#34;max-height:480px; width:auto;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;Its geometry-consistent design further allows strong generalization and controllability over diverse camera motions.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;video-pose.gif&#34;
       alt=&#34;Pose control&#34;
       style=&#34;max-height:480px; width:auto;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;We also introduce &lt;strong&gt;Absolute Orientation Encoding&lt;/strong&gt; to eliminate the ambiguity in pitch and roll in previous T2V methods.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;video-orientation.gif&#34;
       alt=&#34;Orientation control&#34;
       style=&#34;max-height:480px; width:auto;&#34;&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce &lt;strong&gt;Relative Ray Encoding&lt;/strong&gt;, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for &lt;strong&gt;Absolute Orientation Encoding&lt;/strong&gt;, enabling full control over the initial camera orientation. Together, these designs form &lt;strong&gt;UCPE (Unified Camera Positional Encoding)&lt;/strong&gt;, which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding &lt;strong&gt;less than 1% trainable parameters&lt;/strong&gt; while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;














&lt;figure  id=&#34;figure-spherical-camera-optical-flow-the-optical-flow-from-a-panoramic-video-left-can-be-interpreted-as-a-spherical-camera-optical-flow-right-for-complex-motion-f-the-camera-rotation-yields-an-analytic-rotation-flow-fr-on-the-sphere-by-decomposing-f-into-fr-and-its-residual-we-obtain-a-derotated-flow-fd-that-more-clearly-captures-camera-translation-and-object-dynamics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**Spherical Camera Optical Flow.** The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion **f**, the camera rotation yields an analytic rotation flow **fr** on the sphere. By decomposing **f** into **fr** and its residual, we obtain a derotated flow **fd** that more clearly captures camera translation and object dynamics.&#34; srcset=&#34;
               /publication/ucpe/rays_huf96a726b94f0773c18e7799673545a3c_285521_f76fc91118a1f9d20711dd25a31ad0e9.png 400w,
               /publication/ucpe/rays_huf96a726b94f0773c18e7799673545a3c_285521_244bbb45294d02e8da834643011dfe03.png 760w,
               /publication/ucpe/rays_huf96a726b94f0773c18e7799673545a3c_285521_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/ucpe/rays_huf96a726b94f0773c18e7799673545a3c_285521_f76fc91118a1f9d20711dd25a31ad0e9.png&#34;
               width=&#34;600&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Spherical Camera Optical Flow.&lt;/strong&gt; The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion &lt;strong&gt;f&lt;/strong&gt;, the camera rotation yields an analytic rotation flow &lt;strong&gt;fr&lt;/strong&gt; on the sphere. By decomposing &lt;strong&gt;f&lt;/strong&gt; into &lt;strong&gt;fr&lt;/strong&gt; and its residual, we obtain a derotated flow &lt;strong&gt;fd&lt;/strong&gt; that more clearly captures camera translation and object dynamics.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-overview-of-spatial-attention-adapter-the-adapter-injects-ucpe-into-pretrained-transformers-through-a-lightweight-branch-that-preserves-pretrained-priors-it-constructs-hybrid-encoding-from-the-world-to-ray-transform-trw-and-an-optional-lat-up-map-applies-them-within-attention-and-fuses-the-resulting-camera-aware-tokens-back-through-a-zero-initialized-linear-layer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**Overview of Spatial Attention Adapter.** The adapter injects UCPE into pretrained Transformers through a lightweight branch that preserves pretrained priors. It constructs hybrid encoding from the world-to-ray transform **T**^rw and an optional Lat-Up map, applies them within attention, and fuses the resulting camera-aware tokens back through a zero-initialized linear layer.&#34; srcset=&#34;
               /publication/ucpe/pipeline_hu37d829d2573ae743d96282aa90f1e0a6_209628_82e714677e536f631b4063378e1ec42f.png 400w,
               /publication/ucpe/pipeline_hu37d829d2573ae743d96282aa90f1e0a6_209628_9e87f62395826ca4cb8b622ed3f94666.png 760w,
               /publication/ucpe/pipeline_hu37d829d2573ae743d96282aa90f1e0a6_209628_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/ucpe/pipeline_hu37d829d2573ae743d96282aa90f1e0a6_209628_82e714677e536f631b4063378e1ec42f.png&#34;
               width=&#34;600&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Overview of Spatial Attention Adapter.&lt;/strong&gt; The adapter injects UCPE into pretrained Transformers through a lightweight branch that preserves pretrained priors. It constructs hybrid encoding from the world-to-ray transform &lt;strong&gt;T&lt;/strong&gt;^rw and an optional Lat-Up map, applies them within attention, and fuses the resulting camera-aware tokens back through a zero-initialized linear layer.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>PanFlow: Decoupled Motion Control for Panoramic Video Generation</title>
      <link>https://chengzhag.github.io/publication/panflow/</link>
      <pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/panflow/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headeraaai-2026div&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;AAAI 2026&lt;/div&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/hw-liang&#34; target=&#34;_blank&#34;&gt;Hanwen Liang&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://donydchen.github.io&#34; target=&#34;_blank&#34;&gt;Donny Y. Chen&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wuqianyi.top&#34; target=&#34;_blank&#34;&gt;Qianyi Wu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://www.ece.utoronto.ca/people/plataniotis-k-n/&#34; target=&#34;_blank&#34;&gt;Konstantinos N. Plataniotis&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC&lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://www.utoronto.ca&#34; target=&#34;_blank&#34;&gt;University of Toronto&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;!-- &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://github.com/chengzhag/PanFlow&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;a href=&#34;https://www.youtube.com/watch?v=sFTWwlHjNtg&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Video
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2512.00832&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2512.00832&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sFTWwlHjNtg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of &lt;u&gt;pan&lt;/u&gt;oramas to decouple the highly dynamic camera rotation from the input optical &lt;u&gt;flow&lt;/u&gt; condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;














&lt;figure  id=&#34;figure-spherical-camera-optical-flow-the-optical-flow-from-a-panoramic-video-left-can-be-interpreted-as-a-spherical-camera-optical-flow-right-for-complex-motion-f-the-camera-rotation-yields-an-analytic-rotation-flow-fr-on-the-sphere-by-decomposing-f-into-fr-and-its-residual-we-obtain-a-derotated-flow-fd-that-more-clearly-captures-camera-translation-and-object-dynamics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**Spherical Camera Optical Flow.** The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion **f**, the camera rotation yields an analytic rotation flow **f**r on the sphere. By decomposing **f** into **f**r and its residual, we obtain a derotated flow **f**d that more clearly captures camera translation and object dynamics.&#34; srcset=&#34;
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1372630565d310e580904b411bf67150.png 400w,
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_fdb1a4d12b2c667dbba09abc80b9d109.png 760w,
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1372630565d310e580904b411bf67150.png&#34;
               width=&#34;480&#34;
               height=&#34;639&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Spherical Camera Optical Flow.&lt;/strong&gt; The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion &lt;strong&gt;f&lt;/strong&gt;, the camera rotation yields an analytic rotation flow &lt;strong&gt;f&lt;/strong&gt;r on the sphere. By decomposing &lt;strong&gt;f&lt;/strong&gt; into &lt;strong&gt;f&lt;/strong&gt;r and its residual, we obtain a derotated flow &lt;strong&gt;f&lt;/strong&gt;d that more clearly captures camera translation and object dynamics.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-our-proposed-panflow-pipeline-given-an-input-image-and-text-prompt-panflow-uses-a-decoupled-motion-from-a-video-as-reference-to-generate-a-panoramic-video-we-first-estimate-a-decoupled-optical-flow-from-the-reference-video-of-which-the-derotated-flow-is-used-to-generate-a-latent-noise-with-spherical-noise-warping-the-latent-noise-then-serves-as-a-motion-condition-for-a-video-diffusion-transformer-with-lora-fine-tuning-to-generate-derotated-videos-finally-the-decoupled-rotation-is-accumulated-and-applied-to-the-generated-video-frames-to-recover-the-full-motion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**Our proposed PanFlow pipeline.** Given an input image and text prompt, PanFlow uses a decoupled motion from a video as reference to generate a panoramic video. We first estimate a decoupled optical flow from the reference video, of which the derotated flow is used to generate a latent noise with spherical noise warping. The latent noise then serves as a motion condition for a video diffusion transformer with LoRA fine-tuning to generate derotated videos. Finally, the decoupled rotation is accumulated and applied to the generated video frames to recover the full motion.&#34; srcset=&#34;
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_d1da4b9c5bc8b0eba7fea6e3bcbf8081.png 400w,
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_01e40490b605386007714678e24656ee.png 760w,
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_d1da4b9c5bc8b0eba7fea6e3bcbf8081.png&#34;
               width=&#34;1000&#34;
               height=&#34;211&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Our proposed PanFlow pipeline.&lt;/strong&gt; Given an input image and text prompt, PanFlow uses a decoupled motion from a video as reference to generate a panoramic video. We first estimate a decoupled optical flow from the reference video, of which the derotated flow is used to generate a latent noise with spherical noise warping. The latent noise then serves as a motion condition for a video diffusion transformer with LoRA fine-tuning to generate derotated videos. Finally, the decoupled rotation is accumulated and applied to the generated video frames to recover the full motion.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_066b1b0677397289d57a4724f0ee2abd.gif 400w,
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_e9dfb9be0ff120985a2ad02e3b4aac82.gif 760w,
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_066b1b0677397289d57a4724f0ee2abd.gif&#34;
               width=&#34;90%&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;
&lt;p&gt;By conditioning diffusion on spherical-warped motion noise, PanFlow enables precise motion control, produces loop-consistent panoramas, and supports applications such as motion transfer:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;transfer.gif&#34; alt=&#34;transfer&#34; width=&#34;860&#34;&gt;
&lt;/p&gt;
&lt;p&gt;and panoramic video editing:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;editing.gif&#34; alt=&#34;editing&#34; width=&#34;860&#34;&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting</title>
      <link>https://chengzhag.github.io/publication/pansplat/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/pansplat/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headercvpr-2025div&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;CVPR 2025&lt;/div&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://haofeixu.github.io&#34; target=&#34;_blank&#34;&gt;Haofei Xu&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wuqianyi.top&#34; target=&#34;_blank&#34;&gt;Qianyi Wu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://dinhphung.ml&#34; target=&#34;_blank&#34;&gt;Dinh Phung&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC, Caulfield East, Victoria, Australia&lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34;&gt;ETH Zurich&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;!-- &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://github.com/chengzhag/PanSplat&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.youtube.com/watch?v=Kg0du7mFu60&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;short-video&#34;&gt;Short Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/R3qIzL77ZSc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the advent of portable 360Â° cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving.
As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential.
Nevertheless, existing methods are typically constrained to lower resolutions (512 Ã— 1024) due to demanding memory and computational requirements.
In this paper, we present &lt;strong&gt;PanSplat&lt;/strong&gt;, a generalizable, feed-forward approach that efficiently supports &lt;strong&gt;resolution up to 4K&lt;/strong&gt; (2048 Ã— 4096).
Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy.
To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU.
Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;full-video&#34;&gt;Full Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/77G9AQkweg0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-pansplat-pipeline-given-two-wide-baseline-panoramas-we-first-construct-a-hierarchical-spherical-cost-volume-using-a-transformer-based-fpn-to-extract-feature-pyramid-and-2d-u-nets-to-integrate-monocular-depth-priors-for-cost-volume-refinement-we-then-build-gaussian-heads-to-generate-a-feature-pyramid-which-is-later-sampled-with-fibonacci-lattice-and-transformed-to-spherical-3d-gaussian-pyramid-finally-we-unproject-the-gaussian-parameters-for-each-level-and-view-consolidate-them-into-a-global-representation-and-splat-it-into-novel-views-using-a-cubemap-renderer-for-simplicity-intermediate-results-of-only-a-single-view-are-shown&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed PanSplat pipeline. Given two wide-baseline panoramas, we first construct a hierarchical spherical cost volume using a Transformer-based FPN to extract feature pyramid and 2D U-Nets to integrate monocular depth priors for cost volume refinement. We then build Gaussian heads to generate a feature pyramid, which is later sampled with Fibonacci lattice and transformed to spherical 3D Gaussian pyramid. Finally, we unproject the Gaussian parameters for each level and view, consolidate them into a global representation, and splat it into novel views using a cubemap renderer. For simplicity, intermediate results of only a single view are shown.&#34; srcset=&#34;
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_7e2bd5dc1b79802b2b0f980fcb653cfd.png 400w,
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_a952ef3b115c64bb64e2157e43f5d7a7.png 760w,
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_7e2bd5dc1b79802b2b0f980fcb653cfd.png&#34;
               width=&#34;80%&#34;
               height=&#34;187&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed PanSplat pipeline. Given two wide-baseline panoramas, we first construct a hierarchical spherical cost volume using a Transformer-based FPN to extract feature pyramid and 2D U-Nets to integrate monocular depth priors for cost volume refinement. We then build Gaussian heads to generate a feature pyramid, which is later sampled with Fibonacci lattice and transformed to spherical 3D Gaussian pyramid. Finally, we unproject the Gaussian parameters for each level and view, consolidate them into a global representation, and splat it into novel views using a cubemap renderer. For simplicity, intermediate results of only a single view are shown.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;interactive-demo&#34;&gt;Interactive Demo&lt;/h2&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9bKZA2zxAbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

(Best viewed on a desktop browser or Youtube app.)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_297f179c8e43cb969457540e33f5f78a.png 400w,
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_65c8c1a847e40e086f1a35c9b8961378.png 760w,
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_297f179c8e43cb969457540e33f5f78a.png&#34;
               width=&#34;90%&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Taming Stable Diffusion for Text to 360Â° Panorama Image Generation</title>
      <link>https://chengzhag.github.io/publication/panfusion/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/panfusion/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headercvpr-2024-highlightdivhttpscvprthecvfcomconferences2024&#34;&gt;&lt;a href=&#34;https://cvpr.thecvf.com/Conferences/2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;CVPR 2024 Highlight&lt;/div&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wuqianyi.top&#34; target=&#34;_blank&#34;&gt;Qianyi Wu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://xiaoshuihuang.github.io&#34; target=&#34;_blank&#34;&gt;Xiaoshui Huang&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://dinhphung.ml&#34; target=&#34;_blank&#34;&gt;Dinh Phung&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wlouyang.github.io&#34; target=&#34;_blank&#34;&gt;Wanli Ouyang&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://www.shlab.org.cn&#34; target=&#34;_blank&#34;&gt;Shanghai AI Laboratory&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC, Caulfield East, Victoria, Australia&lt;/a&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt;
  &lt;a href=&#34;https://github.com/chengzhag/PanFusion&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.youtube.com/watch?v=Kg0du7mFu60&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2404.07949&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2404.07949.pdf&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/panfusion/teaser_hu23e90f8fc396241f8530cb1ea731a87a_1860576_0928c4c585427def2d388ebe77f14275.png 400w,
               /publication/panfusion/teaser_hu23e90f8fc396241f8530cb1ea731a87a_1860576_f207e05f78e9893810166e7770e29f1c.png 760w,
               /publication/panfusion/teaser_hu23e90f8fc396241f8530cb1ea731a87a_1860576_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panfusion/teaser_hu23e90f8fc396241f8530cb1ea731a87a_1860576_0928c4c585427def2d388ebe77f14275.png&#34;
               width=&#34;90%&#34;
               height=&#34;312&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at &lt;a href=&#34;https://chengzhag.github.io/publication/panfusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chengzhag.github.io/publication/panfusion&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-dual-branch-panfusion-pipeline-the-panorama-branch-upper-provides-global-layout-guidance-and-registers-the-perspective-information-to-get-seamless-panorama-output-the-perspective-branch-lower-harnesses-the-rich-prior-knowledge-of-stable-diffusion-sd-and-provides-guidance-to-alleviate-distortion-under-perspective-projection-both-branches-employ-the-same-unet-backbone-with-shared-weights-while-finetuned-with-separate-lora-layers-equirectangular-perspective-projection-attention-eppa-modules-are-plugged-into-different-layers-of-the-unet-to-pass-information-between-the-two-branches&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed dual-branch PanFusion pipeline. The panorama branch (upper) provides global layout guidance and registers the perspective information to get seamless panorama output. The perspective branch (lower) harnesses the rich prior knowledge of Stable Diffusion (SD) and provides guidance to alleviate distortion under perspective projection. Both branches employ the same UNet backbone with shared weights, while finetuned with separate LoRA layers. Equirectangular-Perspective Projection Attention (EPPA) modules are plugged into different layers of the UNet to pass information between the two branches.&#34; srcset=&#34;
               /publication/panfusion/pipeline_hu5f367bf169fefbabfa80074700fc729f_661292_326e05803f6d9f1e5ffd2f4213e4ea06.png 400w,
               /publication/panfusion/pipeline_hu5f367bf169fefbabfa80074700fc729f_661292_da9ba806b6797c65f763d5a92a23b4fb.png 760w,
               /publication/panfusion/pipeline_hu5f367bf169fefbabfa80074700fc729f_661292_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panfusion/pipeline_hu5f367bf169fefbabfa80074700fc729f_661292_326e05803f6d9f1e5ffd2f4213e4ea06.png&#34;
               width=&#34;80%&#34;
               height=&#34;280&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed dual-branch PanFusion pipeline. The panorama branch (upper) provides global layout guidance and registers the perspective information to get seamless panorama output. The perspective branch (lower) harnesses the rich prior knowledge of Stable Diffusion (SD) and provides guidance to alleviate distortion under perspective projection. Both branches employ the same UNet backbone with shared weights, while finetuned with separate LoRA layers. Equirectangular-Perspective Projection Attention (EPPA) modules are plugged into different layers of the UNet to pass information between the two branches.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;comparisons&#34;&gt;Comparisons&lt;/h2&gt;
&lt;center&gt;
  









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_05.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_05_hu700275244aab45103b63a9bdebf71928_853380_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_05.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_06.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_06_hu700275244aab45103b63a9bdebf71928_912323_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_06.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_07.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_07_hu700275244aab45103b63a9bdebf71928_745484_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_07.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_08.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_08_hu700275244aab45103b63a9bdebf71928_990152_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_08.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_09.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_09_hu700275244aab45103b63a9bdebf71928_1072684_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_09.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_10.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_10_hu700275244aab45103b63a9bdebf71928_1114310_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_10.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_11.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_11_hu700275244aab45103b63a9bdebf71928_873045_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_11.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_12.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_12_hu700275244aab45103b63a9bdebf71928_892583_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_12.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_13.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_13_hu700275244aab45103b63a9bdebf71928_786101_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_13.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-comp&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_14.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/comp/629_supp_Page_14_hu700275244aab45103b63a9bdebf71928_926780_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_14.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;center&gt;
  









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-generalization&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_17.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_17_hu700275244aab45103b63a9bdebf71928_1248988_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_17.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-generalization&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_18.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_18_hu700275244aab45103b63a9bdebf71928_1174457_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_18.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-generalization&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_19.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_19_hu700275244aab45103b63a9bdebf71928_1171826_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_19.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-generalization&#34; href=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_20.jpg&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/panfusion/generalization/629_supp_Page_20_hu700275244aab45103b63a9bdebf71928_1066654_0x190_resize_q75_lanczos.jpg&#34; loading=&#34;lazy&#34; alt=&#34;629_supp_Page_20.jpg&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization</title>
      <link>https://chengzhag.github.io/publication/dpc/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/dpc/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headericcv-2021-oraldivhttpiccv2021thecvfcomhome&#34;&gt;&lt;a href=&#34;http://iccv2021.thecvf.com/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;ICCV 2021 Oral&lt;/div&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://zhpcui.github.io/&#34; target=&#34;_blank&#34;&gt;Zhaopeng Cui&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/MurrayC7&#34; target=&#34;_blank&#34;&gt;Cai Chen&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;http://www.liushuaicheng.org/&#34; target=&#34;_blank&#34;&gt;Shuaicheng Liu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://scholar.google.com/citations?user=4y0QncgAAAAJ&amp;hl=en&#34; target=&#34;_blank&#34;&gt;Bing Zeng&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;http://www.cad.zju.edu.cn/home/bao/&#34; target=&#34;_blank&#34;&gt;Hujun Bao&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.zhangyinda.com/&#34; target=&#34;_blank&#34;&gt;Yinda Zhang&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://en.uestc.edu.cn/&#34; target=&#34;_blank&#34;&gt;University of Electronic Science and Technology of China&lt;/a&gt; 
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;http://www.cad.zju.edu.cn/english.html&#34; target=&#34;_blank&#34;&gt;State Key Lab of CAD &amp; CG, Zhejiang University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://www.ai.google/&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt;
&lt;/div&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/teaser_hucb88ee454679c854cbd5711aaf4cf21d_726202_b9a63cebf26ae21d1f81adcd7a843a0f.png 400w,
               /publication/dpc/teaser_hucb88ee454679c854cbd5711aaf4cf21d_726202_7a2b8742a70b6bdfea1c46b18ed58ffd.png 760w,
               /publication/dpc/teaser_hucb88ee454679c854cbd5711aaf4cf21d_726202_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/teaser_hucb88ee454679c854cbd5711aaf4cf21d_726202_b9a63cebf26ae21d1f81adcd7a843a0f.png&#34;
               width=&#34;90%&#34;
               height=&#34;253&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Panorama images have a much larger field-of-view thus naturally encode enriched scene context information compared to standard perspective images, which however is not well exploited in the previous scene understanding methods.
In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room layout and the shape, pose, position, and semantic category for each object from a single full-view panorama image.
In order to fully utilize the rich context information, we design a novel graph neural network based context model to predict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly.
Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding.
Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geometry accuracy and object arrangement.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/mO1EtUHnX4w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;
&lt;center&gt;
  









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_01.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_01_hu5f1572c67a6f7cc7201698862c58c249_352891_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;07881_é¡µé¢_01.png&#34; width=&#34;134&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_03.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_03_hu35a9a6da8ea7856690f21d9c68501222_750444_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;07881_é¡µé¢_03.png&#34; width=&#34;134&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_07.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_07_hu092b48727c0cf5fdb7020edb7dde2be4_8126158_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;07881_é¡µé¢_07.png&#34; width=&#34;134&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_08.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/dpc/pages/07881_%E9%A1%B5%E9%9D%A2_08_hu5f1572c67a6f7cc7201698862c58c249_389067_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;07881_é¡µé¢_08.png&#34; width=&#34;134&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;/center&gt;
&lt;center&gt;
  &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt;
  &lt;a href=&#34;https://github.com/chengzhag/DeepPanoContext&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;a href=&#34;https://www.youtube.com/watch?v=mO1EtUHnX4w&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1JU4y1w7Yu/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2108.10743&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  arXiv (Supp included)
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2108.10743&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;figure&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34;&gt;
        &lt;img alt=&#34;Our proposed pipeline. We first do a bottom-up initialization with several SoTA methods and provide various features, including geometric, semantic, and appearance features of objects and layout. These are then fed into our proposed RGCN network to refine the initial object pose and estimate the relation among objects and layout. A relation optimization is adopted afterward to further adjust the 3d object arrangement to align with the 2D observation, conform with the predicted relation, and resolve physical collision.&#34; srcset=&#34;
               pipeline_anim.gif&#34; src=&#34;pipeline_anim.gif&#34; width=&#34;90%&#34; loading=&#34;lazy&#34; data-zoomable=&#34;&#34; class=&#34;medium-zoom-image&#34;&gt;&lt;/div&gt;
  &lt;/div&gt;
  &lt;figcaption&gt;
      Our proposed pipeline. We first do a bottom-up initialization with several SoTA methods and provide various features, including geometric, semantic, and appearance features of objects and layout. These are then fed into our proposed RGCN network to refine the initial object pose and estimate the relation among objects and layout. A relation optimization is adopted afterward to further adjust the 3d object arrangement to align with the 2D observation, conform with the predicted relation, and resolve physical collision.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We first extract the whole-room layout under Manhattan World assumption and the initial object estimates including locations, sizes, poses, semantic categories, and latent shape codes.
These, along with extracted features, are then fed into the Relation-based Graph Convolutional Network (RGCN) for refinement and to estimate relations among objects and layout simultaneously.
Then, a differentiable Relation Optimization (RO) based on physical violation, observation, and relation is proposed to resolve collisions and adjust object poses.
Finally, the 3D shape is recovered by feeding the latent shape code into Local Implicit Deep Function (LDIF), and combined with object pose and room layout to achieve total scene understanding.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;interactive-results&#34;&gt;Interactive Results&lt;/h2&gt;
&lt;!-- model-viewer css --&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;model-viewer.css&#34;&gt;
&lt;!-- Import the component --&gt;
&lt;script type=&#34;module&#34; src=&#34;https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js&#34;&gt;&lt;/script&gt;
&lt;center&gt;
  &lt;div class=&#39;container&#39;&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Input&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Detection and Layout&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Reconstruction&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/input/input-Beechwood_1_int-00094-rgb_hu1f1421bf608bff761f5cad7df635da0a_142383_f0451d9b53e541db5377fa872d1f53ee.jpg 400w,
               /publication/dpc/input/input-Beechwood_1_int-00094-rgb_hu1f1421bf608bff761f5cad7df635da0a_142383_43fd2df5f15e528c1a7e75dc37bb8680.jpg 760w,
               /publication/dpc/input/input-Beechwood_1_int-00094-rgb_hu1f1421bf608bff761f5cad7df635da0a_142383_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/input/input-Beechwood_1_int-00094-rgb_hu1f1421bf608bff761f5cad7df635da0a_142383_f0451d9b53e541db5377fa872d1f53ee.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/det3d/Ours-Beechwood_1_int-00094-det3d_hu93d8c63e1c0851117f49de9289f004b4_204110_cb572b852efe688193d9476259772a36.jpg 400w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00094-det3d_hu93d8c63e1c0851117f49de9289f004b4_204110_ccab5268094829a9bbd70507651221fc.jpg 760w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00094-det3d_hu93d8c63e1c0851117f49de9289f004b4_204110_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/det3d/Ours-Beechwood_1_int-00094-det3d_hu93d8c63e1c0851117f49de9289f004b4_204110_cb572b852efe688193d9476259772a36.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;recon/Ours-Beechwood_1_int-00094-rgb.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;629.2deg 46.87deg 9m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/input/input-Merom_1_int-00086-rgb_hueefa868c6405496793cc8591e2bea1ad_169365_09e12c344faeedf315c6f95a8b72e285.jpg 400w,
               /publication/dpc/input/input-Merom_1_int-00086-rgb_hueefa868c6405496793cc8591e2bea1ad_169365_851da08d12f8552ee8f5cac12e45c4fa.jpg 760w,
               /publication/dpc/input/input-Merom_1_int-00086-rgb_hueefa868c6405496793cc8591e2bea1ad_169365_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/input/input-Merom_1_int-00086-rgb_hueefa868c6405496793cc8591e2bea1ad_169365_09e12c344faeedf315c6f95a8b72e285.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/det3d/Ours-Merom_1_int-00086-det3d_hu272b7a6012d195e7c1a626eed54ff38a_250774_733bae25648019cb6a9e9f3eb7eb755d.jpg 400w,
               /publication/dpc/det3d/Ours-Merom_1_int-00086-det3d_hu272b7a6012d195e7c1a626eed54ff38a_250774_08fed0639863ef90d2033f378163d482.jpg 760w,
               /publication/dpc/det3d/Ours-Merom_1_int-00086-det3d_hu272b7a6012d195e7c1a626eed54ff38a_250774_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/det3d/Ours-Merom_1_int-00086-det3d_hu272b7a6012d195e7c1a626eed54ff38a_250774_733bae25648019cb6a9e9f3eb7eb755d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;recon/Ours-Merom_1_int-00086-rgb.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;540.6deg 40.78deg 11m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/input/input-Beechwood_1_int-00089-rgb_hu305647bc122b9941d0da3adc41c23000_156265_3172b1692d45cf2a18f13ac887d43b95.jpg 400w,
               /publication/dpc/input/input-Beechwood_1_int-00089-rgb_hu305647bc122b9941d0da3adc41c23000_156265_522082302fc409ba2e640dd9a5cf5ae7.jpg 760w,
               /publication/dpc/input/input-Beechwood_1_int-00089-rgb_hu305647bc122b9941d0da3adc41c23000_156265_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/input/input-Beechwood_1_int-00089-rgb_hu305647bc122b9941d0da3adc41c23000_156265_3172b1692d45cf2a18f13ac887d43b95.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/det3d/Ours-Beechwood_1_int-00089-det3d_hu82c450f009c646843927e92cb95076a4_230297_f83a0ce9069cb9340e04a0dfbbd9b02c.jpg 400w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00089-det3d_hu82c450f009c646843927e92cb95076a4_230297_e5f37aa71926033bfb8232a34c021026.jpg 760w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00089-det3d_hu82c450f009c646843927e92cb95076a4_230297_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/det3d/Ours-Beechwood_1_int-00089-det3d_hu82c450f009c646843927e92cb95076a4_230297_f83a0ce9069cb9340e04a0dfbbd9b02c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;recon/Ours-Beechwood_1_int-00089-rgb.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;540.6deg 40.78deg 9m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/input/input-Merom_1_int-00070-rgb_hu207d8c1f7d804600ffff7094538af2e3_182182_843599a3c3d2e33ed5ca9d57c120e8db.jpg 400w,
               /publication/dpc/input/input-Merom_1_int-00070-rgb_hu207d8c1f7d804600ffff7094538af2e3_182182_9c086e7d353f83ffbd43f95fc92607f8.jpg 760w,
               /publication/dpc/input/input-Merom_1_int-00070-rgb_hu207d8c1f7d804600ffff7094538af2e3_182182_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/input/input-Merom_1_int-00070-rgb_hu207d8c1f7d804600ffff7094538af2e3_182182_843599a3c3d2e33ed5ca9d57c120e8db.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/det3d/Ours-Merom_1_int-00070-det3d_hu1bb63e02eb2453effdda2d158667711a_279806_f60fa41ecf1d18ffaabcda8ccb56c936.jpg 400w,
               /publication/dpc/det3d/Ours-Merom_1_int-00070-det3d_hu1bb63e02eb2453effdda2d158667711a_279806_94fd2432bedc3f166b999a635e9ae935.jpg 760w,
               /publication/dpc/det3d/Ours-Merom_1_int-00070-det3d_hu1bb63e02eb2453effdda2d158667711a_279806_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/det3d/Ours-Merom_1_int-00070-det3d_hu1bb63e02eb2453effdda2d158667711a_279806_f60fa41ecf1d18ffaabcda8ccb56c936.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;recon/Ours-Merom_1_int-00070-rgb.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;809.2deg 39.67deg 9m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/input/input-Beechwood_1_int-00069-rgb_hud8ce74b05597b8f53669aa2be60d40e5_181911_8dff108e413a5fcba4bca46cb81051be.jpg 400w,
               /publication/dpc/input/input-Beechwood_1_int-00069-rgb_hud8ce74b05597b8f53669aa2be60d40e5_181911_6b414d69b0dde67c32d06be69d6a314e.jpg 760w,
               /publication/dpc/input/input-Beechwood_1_int-00069-rgb_hud8ce74b05597b8f53669aa2be60d40e5_181911_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/input/input-Beechwood_1_int-00069-rgb_hud8ce74b05597b8f53669aa2be60d40e5_181911_8dff108e413a5fcba4bca46cb81051be.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;pano&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/dpc/det3d/Ours-Beechwood_1_int-00069-det3d_huf9f7a7dbd3c4969d7a0a7461b516f8cf_276890_d2118c51c9d8e083cd0ab3b2982f097b.jpg 400w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00069-det3d_huf9f7a7dbd3c4969d7a0a7461b516f8cf_276890_1a0d2ca385f863027ab99c54b98105a8.jpg 760w,
               /publication/dpc/det3d/Ours-Beechwood_1_int-00069-det3d_huf9f7a7dbd3c4969d7a0a7461b516f8cf_276890_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/det3d/Ours-Beechwood_1_int-00069-det3d_huf9f7a7dbd3c4969d7a0a7461b516f8cf_276890_d2118c51c9d8e083cd0ab3b2982f097b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;recon/Ours-Beechwood_1_int-00069-rgb.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;540.6deg 40.78deg 9m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-qualitative-comparison-on-3d-object-detection-and-scene-reconstruction-we-compare-object-detection-and-compare-scene-reconstruction-results-with-total3d-pers-and-im3d-pers-in-both-birds-eye-view-and-panorama-format&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Qualitative comparison on 3D object detection and scene reconstruction. We compare object detection and compare scene reconstruction results with Total3D-Pers and Im3D-Pers in both bird&amp;#39;s eye view and panorama format.&#34; srcset=&#34;
               /publication/dpc/results_hu3e44127679b5aa673fe84eb09188cd51_2521313_7b6f8801e7d104730ba955cd8f31dd38.png 400w,
               /publication/dpc/results_hu3e44127679b5aa673fe84eb09188cd51_2521313_3c6d910f22fa6ff7ad1cbba56831ee80.png 760w,
               /publication/dpc/results_hu3e44127679b5aa673fe84eb09188cd51_2521313_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/dpc/results_hu3e44127679b5aa673fe84eb09188cd51_2521313_7b6f8801e7d104730ba955cd8f31dd38.png&#34;
               width=&#34;90%&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qualitative comparison on 3D object detection and scene reconstruction. We compare object detection and compare scene reconstruction results with Total3D-Pers and Im3D-Pers in both bird&amp;rsquo;s eye view and panorama format.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Holistic 3D Scene Understanding from a Single Image with Implicit Representation</title>
      <link>https://chengzhag.github.io/publication/im3d/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/im3d/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headercvpr-2021divhttpcvpr2021thecvfcom&#34;&gt;&lt;a href=&#34;http://cvpr2021.thecvf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;CVPR 2021&lt;/div&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;*
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://zhpcui.github.io/&#34; target=&#34;_blank&#34;&gt;Zhaopeng Cui&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;*
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.zhangyinda.com/&#34; target=&#34;_blank&#34;&gt;Yinda Zhang&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;*
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://scholar.google.com/citations?user=4y0QncgAAAAJ&amp;hl=en&#34; target=&#34;_blank&#34;&gt;Bing Zeng&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://people.inf.ethz.ch/pomarc/&#34; target=&#34;_blank&#34;&gt;Marc Pollefeys&lt;/a&gt;
  &lt;sup&gt;4&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;http://www.liushuaicheng.org/&#34; target=&#34;_blank&#34;&gt;Shuaicheng Liu&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;http://www.cad.zju.edu.cn/english.html&#34; target=&#34;_blank&#34;&gt;State Key Lab of CAD &amp; CG, Zhejiang University&lt;/a&gt; 
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://en.uestc.edu.cn/&#34; target=&#34;_blank&#34;&gt;University of Electronic Science and Technology of China&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://www.ai.google/&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;4&lt;/sup&gt;
  &lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34;&gt;ETH Zurich&lt;/a&gt; 
&lt;/div&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_e9d3a917824ba8aeb1bfb7d5a213cbde.png 400w,
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_ead458b3d2b59f5730be03c9ec34c976.png 760w,
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_e9d3a917824ba8aeb1bfb7d5a213cbde.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.&lt;/p&gt;
&lt;!-- ## 3D Scene Understanding 
Given a single color image,
- Estimate the room layout, including object categories and poses in 3D space
- Reconstruct mesh of individual object --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Kg0du7mFu60&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;
&lt;!-- ![page1](02192_é¡µé¢_01.png)![page3](02192_é¡µé¢_03.png)![page5](02192_é¡µé¢_05.png)![page7](02192_é¡µé¢_07.png) --&gt;
&lt;center&gt;
  









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_01.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_01_hu7b4e8c585c78596d52f33bba49723711_179891_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_é¡µé¢_01.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_03.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_03_hu7b4e8c585c78596d52f33bba49723711_221609_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_é¡µé¢_03.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_05.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_05_hu7b4e8c585c78596d52f33bba49723711_201925_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_é¡µé¢_05.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_07.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_07_hu7b4e8c585c78596d52f33bba49723711_293071_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_é¡µé¢_07.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;/center&gt;
&lt;center&gt;
  &lt;!-- &lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34;  class=&#34;btn btn-primary px-3 py-3&#34;&gt;arXiv&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;
 --&gt;
  &lt;!-- &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Paper&lt;/a&gt; --&gt;
  &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt;
  &lt;a href=&#34;https://github.com/pidan1231239/Implicit3DUnderstanding&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;a href=&#34;https://www.youtube.com/watch?v=Kg0du7mFu60&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2103.06422&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;!-- &lt;center&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34;&gt;[arXiv]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://arxiv.org/pdf/2103.06422&#34;&gt;[Paper]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;02192-supp.pdf&#34;&gt;[Supp]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/pidan1231239/Implicit3DUnderstanding&#34;&gt;[GitHub]&lt;/a&gt;
&lt;/center&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients&lt;/li&gt;
&lt;li&gt;And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding&lt;/li&gt;
&lt;li&gt;Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-pipeline-we-initialize-the-layout-estimation-and-3d-object-poses-with-len-and-odn-from-prior-work-then-refine-them-with-scene-graph-convolutional-network-sgcn-we-utilize-a-local-implicit-embedding-network-lien-to-encode-latent-code-for-ldif-decoder-and-to-extract-implicit-features-for-sgcn-with-the-help-of-ldif-and-marching-cube-algorithm-object-meshes-are-extracted-then-rotated-scaled-and-put-into-places-to-construct-the-scene&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.&#34; srcset=&#34;
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png 400w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_ef8517bd9f675e4bedaf968e44c025e5.png 760w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage.
In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry.
The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose.
In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;interactive-results&#34;&gt;Interactive Results&lt;/h2&gt;
&lt;!-- model-viewer css --&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;model-viewer.css&#34;&gt;
&lt;!-- Import the component --&gt;
&lt;script type=&#34;module&#34; src=&#34;https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js&#34;&gt;&lt;/script&gt;
&lt;center&gt;
  &lt;div class=&#39;container&#39;&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Input&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Total3D&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;p class=&#39;header&#39;&gt;Ours&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/002435_hu9f4da8feeb8f488dbc844ceb4d1e5894_31728_34fede712ec055a09eae71d37c699e2e.jpg 400w,
               /publication/im3d/input/002435_hu9f4da8feeb8f488dbc844ceb4d1e5894_31728_77ebae929083f1a1d9b2be5aa852d6a2.jpg 760w,
               /publication/im3d/input/002435_hu9f4da8feeb8f488dbc844ceb4d1e5894_31728_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/002435_hu9f4da8feeb8f488dbc844ceb4d1e5894_31728_34fede712ec055a09eae71d37c699e2e.jpg&#34;
               width=&#34;561&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/2435_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/2435_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/000724_hu60acda178c6c414ae074c5a84b819e9e_44711_afc315577b48789173e155e63118e555.jpg 400w,
               /publication/im3d/input/000724_hu60acda178c6c414ae074c5a84b819e9e_44711_8c571ddedd8f9236f8459eba65c0fd6d.jpg 760w,
               /publication/im3d/input/000724_hu60acda178c6c414ae074c5a84b819e9e_44711_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/000724_hu60acda178c6c414ae074c5a84b819e9e_44711_afc315577b48789173e155e63118e555.jpg&#34;
               width=&#34;730&#34;
               height=&#34;530&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/724_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/724_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/000276_huc6e3ab9fc38f58a9bafe19ad89e08761_43748_69c762dbe7b8ef43ff843cb33162b9e1.jpg 400w,
               /publication/im3d/input/000276_huc6e3ab9fc38f58a9bafe19ad89e08761_43748_06a8cbd97c646a108ee7ef351ca81737.jpg 760w,
               /publication/im3d/input/000276_huc6e3ab9fc38f58a9bafe19ad89e08761_43748_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/000276_huc6e3ab9fc38f58a9bafe19ad89e08761_43748_69c762dbe7b8ef43ff843cb33162b9e1.jpg&#34;
               width=&#34;730&#34;
               height=&#34;530&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/276_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/276_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/001140_hub6d71cf445d4a1d1444ecd3033d99239_60313_1c2a13736ea7778b63371bd59dc8e498.jpg 400w,
               /publication/im3d/input/001140_hub6d71cf445d4a1d1444ecd3033d99239_60313_0f716db03a99c16e509b6a61ce8c883f.jpg 760w,
               /publication/im3d/input/001140_hub6d71cf445d4a1d1444ecd3033d99239_60313_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/001140_hub6d71cf445d4a1d1444ecd3033d99239_60313_1c2a13736ea7778b63371bd59dc8e498.jpg&#34;
               width=&#34;730&#34;
               height=&#34;530&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/1140_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/1140_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2.7m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/000765_hub1750ce7295fa2f68bd853eff6735d57_46395_6f7c1ff71db8293ea59bace74d38b4e1.jpg 400w,
               /publication/im3d/input/000765_hub1750ce7295fa2f68bd853eff6735d57_46395_32a99c7691e042129650abca07910c54.jpg 760w,
               /publication/im3d/input/000765_hub1750ce7295fa2f68bd853eff6735d57_46395_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/000765_hub1750ce7295fa2f68bd853eff6735d57_46395_6f7c1ff71db8293ea59bace74d38b4e1.jpg&#34;
               width=&#34;730&#34;
               height=&#34;530&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/765_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/765_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#39;row&#39; &gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          













&lt;figure class=&#34;input&#34; &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/input/001149_hue52c2dd20dd88036a18bcafe64c56792_54694_06b9fcbb900685fa99e11f48449cc919.jpg 400w,
               /publication/im3d/input/001149_hue52c2dd20dd88036a18bcafe64c56792_54694_346b1f8786db404c8b187198b71812e7.jpg 760w,
               /publication/im3d/input/001149_hue52c2dd20dd88036a18bcafe64c56792_54694_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/input/001149_hue52c2dd20dd88036a18bcafe64c56792_54694_06b9fcbb900685fa99e11f48449cc919.jpg&#34;
               width=&#34;730&#34;
               height=&#34;530&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;total3d/1149_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class =&#39;column&#39;&gt;
        &lt;div id=&#34;card&#34;&gt;
          &lt;model-viewer src=&#34;im3d/1149_mesh.glb&#34; interaction-prompt=&#34;when-focused&#34; camera-controls exposure=&#34;0.72&#34; shadow-intensity=&#34;2.7&#34; shadow-softness=&#34;0.84&#34; camera-target=&#34;2m -0.5m 0.1m&#34; min-camera-orbit=&#34;auto auto auto&#34; max-camera-orbit=&#34;auto auto 11.89m&#34; camera-orbit=&#34;270deg 50deg 8m&#34; field-of-view=&#34;30deg&#34;&gt;
          &lt;/model-viewer&gt; 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-qualitative-comparison-on-object-detection-and-scene-reconstruction-we-compare-object-detection-results-with-total3d-and-ground-truth-in-both-oblique-view-and-camera-view-the-results-show-that-our-method-gives-more-accurate-bounding-box-estimation-and-with-less-intersection-we-compare-scene-reconstruction-results-with-total3d-in-camera-view-and-observe-more-reasonable-object-poses&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.&#34; srcset=&#34;
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png 400w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_74508ae60005474b267bfa284ca2971f.png 760w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png&#34;
               width=&#34;666&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
  </channel>
</rss>
