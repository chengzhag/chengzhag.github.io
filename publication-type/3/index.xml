<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3 | Cheng Zhang</title>
    <link>https://chengzhag.github.io/publication-type/3/</link>
      <atom:link href="https://chengzhag.github.io/publication-type/3/index.xml" rel="self" type="application/rss+xml" />
    <description>3</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengzhag.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>3</title>
      <link>https://chengzhag.github.io/publication-type/3/</link>
    </image>
    
    <item>
      <title>PanFlow: Decoupled Motion Control for Panoramic Video Generation</title>
      <link>https://chengzhag.github.io/publication/panflow/</link>
      <pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/panflow/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headeraaai-2026div&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;AAAI 2026&lt;/div&gt;&lt;/h2&gt;
&lt;!-- - Hanwen Liang
- Donny Y. Chen
- Qianyi Wu
- Konstantinos N. Plataniotis
- Camilo Cruz Gambardella
- Jianfei Cai --&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/hw-liang&#34; target=&#34;_blank&#34;&gt;Hanwen Liang&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://donydchen.github.io&#34; target=&#34;_blank&#34;&gt;Donny Y. Chen&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wuqianyi.top&#34; target=&#34;_blank&#34;&gt;Qianyi Wu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://www.ece.utoronto.ca/people/plataniotis-k-n/&#34; target=&#34;_blank&#34;&gt;Konstantinos N. Plataniotis&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC&lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://www.utoronto.ca&#34; target=&#34;_blank&#34;&gt;University of Toronto&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;!-- &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://github.com/chengzhag/PanSplat&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.youtube.com/watch?v=Kg0du7mFu60&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sFTWwlHjNtg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of &lt;u&gt;pan&lt;/u&gt;oramas to decouple the highly dynamic camera rotation from the input optical &lt;u&gt;flow&lt;/u&gt; condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-spherical-camera-optical-flow-the-optical-flow-from-a-panoramic-video-left-can-be-interpreted-as-a-spherical-camera-optical-flow-right-for-complex-motion-f-the-camera-rotation-yields-an-analytic-rotation-flow-fr-on-the-sphere-by-decomposing-f-into-fr-and-its-residual-we-obtain-a-derotated-flow-fd-that-more-clearly-captures-camera-translation-and-object-dynamics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Spherical Camera Optical Flow. The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion **f**, the camera rotation yields an analytic rotation flow **fr** on the sphere. By decomposing **f** into **fr** and its residual, we obtain a derotated flow **fd** that more clearly captures camera translation and object dynamics.&#34; srcset=&#34;
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1372630565d310e580904b411bf67150.png 400w,
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_fdb1a4d12b2c667dbba09abc80b9d109.png 760w,
               /publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/teaser_hu8f94f8bd176fc76feb23c2a004bde52e_828004_1372630565d310e580904b411bf67150.png&#34;
               width=&#34;50%&#34;
               height=&#34;639&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Spherical Camera Optical Flow. The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion &lt;strong&gt;f&lt;/strong&gt;, the camera rotation yields an analytic rotation flow &lt;strong&gt;fr&lt;/strong&gt; on the sphere. By decomposing &lt;strong&gt;f&lt;/strong&gt; into &lt;strong&gt;fr&lt;/strong&gt; and its residual, we obtain a derotated flow &lt;strong&gt;fd&lt;/strong&gt; that more clearly captures camera translation and object dynamics.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-our-proposed-panflow-pipeline-given-an-input-image-and-text-prompt-panflow-uses-a-decoupled-motion-from-a-video-as-reference-to-generate-a-panoramic-video-we-first-estimate-a-decoupled-optical-flow-from-the-reference-video-of-which-the-derotated-flow-is-used-to-generate-a-latent-noise-with-spherical-noise-warping-the-latent-noise-then-serves-as-a-motion-condition-for-a-video-diffusion-transformer-with-lora-fine-tuning-to-generate-derotated-videos-finally-the-decoupled-rotation-is-accumulated-and-applied-to-the-generated-video-frames-to-recover-the-full-motion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed PanFlow pipeline. Given an input image and text prompt, PanFlow uses a decoupled motion from a video as reference to generate a panoramic video. We first estimate a decoupled optical flow from the reference video, of which the derotated flow is used to generate a latent noise with spherical noise warping. The latent noise then serves as a motion condition for a video diffusion transformer with LoRA fine-tuning to generate derotated videos. Finally, the decoupled rotation is accumulated and applied to the generated video frames to recover the full motion.&#34; srcset=&#34;
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_d1da4b9c5bc8b0eba7fea6e3bcbf8081.png 400w,
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_01e40490b605386007714678e24656ee.png 760w,
               /publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/pipeline_hu56d2780c2ed99b29e4fc3878bdb0433e_1183105_d1da4b9c5bc8b0eba7fea6e3bcbf8081.png&#34;
               width=&#34;80%&#34;
               height=&#34;211&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed PanFlow pipeline. Given an input image and text prompt, PanFlow uses a decoupled motion from a video as reference to generate a panoramic video. We first estimate a decoupled optical flow from the reference video, of which the derotated flow is used to generate a latent noise with spherical noise warping. The latent noise then serves as a motion condition for a video diffusion transformer with LoRA fine-tuning to generate derotated videos. Finally, the decoupled rotation is accumulated and applied to the generated video frames to recover the full motion.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_066b1b0677397289d57a4724f0ee2abd.gif 400w,
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_e9dfb9be0ff120985a2ad02e3b4aac82.gif 760w,
               /publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/panflow/editing_huf1f7b188ed05826a963caeaad294c687_5085472_066b1b0677397289d57a4724f0ee2abd.gif&#34;
               width=&#34;90%&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;
&lt;p&gt;By conditioning diffusion on spherical-warped motion noise, PanFlow enables precise motion control, produces loop-consistent panoramas, and supports applications such as motion transfer:&lt;/p&gt;
&lt;figure&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34;&gt;
        &lt;img src=&#34;transfer.gif&#34; width=&#34;70%&#34; loading=&#34;lazy&#34; data-zoomable=&#34;&#34; class=&#34;medium-zoom-image&#34;&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;and panoramic video editing:&lt;/p&gt;
&lt;figure&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34;&gt;
        &lt;img src=&#34;editing.gif&#34; width=&#34;70%&#34; loading=&#34;lazy&#34; data-zoomable=&#34;&#34; class=&#34;medium-zoom-image&#34;&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting</title>
      <link>https://chengzhag.github.io/publication/pansplat/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/pansplat/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;div-classpublication-headercvpr-2025div&#34;&gt;&lt;div class=&#34;publication-header&#34;&gt;CVPR 2025&lt;/div&gt;&lt;/h2&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://haofeixu.github.io&#34; target=&#34;_blank&#34;&gt;Haofei Xu&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://wuqianyi.top&#34; target=&#34;_blank&#34;&gt;Qianyi Wu&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;a href=&#34;https://www.researchgate.net/profile/Camilo-Cruz-Gambardella&#34; target=&#34;_blank&#34;&gt;Camilo Cruz Gambardella&lt;/a&gt;
  &lt;sup&gt;1,2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://dinhphung.ml&#34; target=&#34;_blank&#34;&gt;Dinh Phung&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://jianfei-cai.github.io&#34; target=&#34;_blank&#34;&gt;Jianfei Cai&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
&lt;/div&gt;
&lt;div class=&#34;publication-header&#34;&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;https://www.monash.edu&#34; target=&#34;_blank&#34;&gt;Monash University&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;!-- &lt;br /&gt; --&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://building4pointzero.org&#34; target=&#34;_blank&#34;&gt;Building 4.0 CRC, Caulfield East, Victoria, Australia&lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34;&gt;ETH Zurich&lt;/a&gt; 
  &lt;!-- &lt;br /&gt; --&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;!-- &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;cite.bib&#34;&gt;
  Cite
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://github.com/chengzhag/PanSplat&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
  &lt;!-- &lt;a href=&#34;https://www.youtube.com/watch?v=Kg0du7mFu60&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  YouTube
  &lt;/a&gt;
  &lt;a href=&#34;https://www.bilibili.com/video/BV1By4y1g7c5/&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  bilibili
  &lt;/a&gt; --&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &lt;a href=&#34;https://arxiv.org/pdf/2412.12096&#34; class=&#34;btn btn-outline-primary&#34;&gt;
  Paper
  &lt;/a&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;short-video&#34;&gt;Short Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/R3qIzL77ZSc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the advent of portable 360° cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving.
As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential.
Nevertheless, existing methods are typically constrained to lower resolutions (512 × 1024) due to demanding memory and computational requirements.
In this paper, we present &lt;strong&gt;PanSplat&lt;/strong&gt;, a generalizable, feed-forward approach that efficiently supports &lt;strong&gt;resolution up to 4K&lt;/strong&gt; (2048 × 4096).
Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy.
To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU.
Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;full-video&#34;&gt;Full Video&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/77G9AQkweg0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-pansplat-pipeline-given-two-wide-baseline-panoramas-we-first-construct-a-hierarchical-spherical-cost-volume-using-a-transformer-based-fpn-to-extract-feature-pyramid-and-2d-u-nets-to-integrate-monocular-depth-priors-for-cost-volume-refinement-we-then-build-gaussian-heads-to-generate-a-feature-pyramid-which-is-later-sampled-with-fibonacci-lattice-and-transformed-to-spherical-3d-gaussian-pyramid-finally-we-unproject-the-gaussian-parameters-for-each-level-and-view-consolidate-them-into-a-global-representation-and-splat-it-into-novel-views-using-a-cubemap-renderer-for-simplicity-intermediate-results-of-only-a-single-view-are-shown&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed PanSplat pipeline. Given two wide-baseline panoramas, we first construct a hierarchical spherical cost volume using a Transformer-based FPN to extract feature pyramid and 2D U-Nets to integrate monocular depth priors for cost volume refinement. We then build Gaussian heads to generate a feature pyramid, which is later sampled with Fibonacci lattice and transformed to spherical 3D Gaussian pyramid. Finally, we unproject the Gaussian parameters for each level and view, consolidate them into a global representation, and splat it into novel views using a cubemap renderer. For simplicity, intermediate results of only a single view are shown.&#34; srcset=&#34;
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_7e2bd5dc1b79802b2b0f980fcb653cfd.png 400w,
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_a952ef3b115c64bb64e2157e43f5d7a7.png 760w,
               /publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/pansplat/pipeline_hu69136d99cddca9f0238467cbbc2c2832_1105566_7e2bd5dc1b79802b2b0f980fcb653cfd.png&#34;
               width=&#34;80%&#34;
               height=&#34;187&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed PanSplat pipeline. Given two wide-baseline panoramas, we first construct a hierarchical spherical cost volume using a Transformer-based FPN to extract feature pyramid and 2D U-Nets to integrate monocular depth priors for cost volume refinement. We then build Gaussian heads to generate a feature pyramid, which is later sampled with Fibonacci lattice and transformed to spherical 3D Gaussian pyramid. Finally, we unproject the Gaussian parameters for each level and view, consolidate them into a global representation, and splat it into novel views using a cubemap renderer. For simplicity, intermediate results of only a single view are shown.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;interactive-demo&#34;&gt;Interactive Demo&lt;/h2&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9bKZA2zxAbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

(Best viewed on a desktop browser or Youtube app.)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_297f179c8e43cb969457540e33f5f78a.png 400w,
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_65c8c1a847e40e086f1a35c9b8961378.png 760w,
               /publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/pansplat/teaser_hue964f49ef3a040669a69db22bafcc8b8_1783743_297f179c8e43cb969457540e33f5f78a.png&#34;
               width=&#34;90%&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
  </channel>
</rss>
