<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Cheng Zhang</title>
    <link>https://chengzhag.github.io/tag/deep-learning/</link>
      <atom:link href="https://chengzhag.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 11 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengzhag.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://chengzhag.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Holistic 3D Scene Understanding from a Single Image with Implicit Representation</title>
      <link>https://chengzhag.github.io/publication/im3d/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/im3d/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;centercvpr-2021centerhttpcvpr2021thecvfcom&#34;&gt;&lt;a href=&#34;http://cvpr2021.thecvf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;center&gt;CVPR 2021&lt;/center&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;center&gt;
  &lt;a href=&#34;https://chengzhag.github.io/&#34; target=&#34;_blank&#34;&gt;Cheng Zhang&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://zhpcui.github.io/&#34; target=&#34;_blank&#34;&gt;Zhaopeng Cui&lt;/a&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://www.zhangyinda.com/&#34; target=&#34;_blank&#34;&gt;Yinda Zhang&lt;/a&gt;
  &lt;sup&gt;3&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://scholar.google.com/citations?user=4y0QncgAAAAJ&amp;hl=en&#34; target=&#34;_blank&#34;&gt;Bing Zeng&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://people.inf.ethz.ch/pomarc/&#34; target=&#34;_blank&#34;&gt;Marc Pollefeys&lt;/a&gt;
  &lt;sup&gt;4&lt;/sup&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;http://www.liushuaicheng.org/&#34; target=&#34;_blank&#34;&gt;Shuaicheng Liu&lt;/a&gt;
  &lt;sup&gt;2&lt;/sup&gt;
&lt;/center&gt;
&lt;center&gt;
  &lt;sup&gt;1&lt;/sup&gt;
  &lt;a href=&#34;http://www.cad.zju.edu.cn/english.html&#34; target=&#34;_blank&#34;&gt;State Key Lab of CAD &amp; CG, Zhejiang University&lt;/a&gt; 
  &lt;!-- &amp;nbsp; &amp;nbsp; --&gt;
  &lt;br /&gt;
  &lt;sup&gt;2&lt;/sup&gt;
  &lt;a href=&#34;https://en.uestc.edu.cn/&#34; target=&#34;_blank&#34;&gt;University of Electronic Science and Technology of China&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;3&lt;/sup&gt;
  &lt;a href=&#34;https://www.ai.google/&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;sup&gt;4&lt;/sup&gt;
  &lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34;&gt;ETH Zurich&lt;/a&gt; 
&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_e9d3a917824ba8aeb1bfb7d5a213cbde.png 400w,
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_ead458b3d2b59f5730be03c9ec34c976.png 760w,
               /publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/featured_hue4f967c6ccd009b15285c6dc5f298fa7_616395_e9d3a917824ba8aeb1bfb7d5a213cbde.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.&lt;/p&gt;
&lt;!-- ## 3D Scene Understanding 
Given a single color image,
- Estimate the room layout, including object categories and poses in 3D space
- Reconstruct mesh of individual object --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;
&lt;!-- ![page1](02192_页面_01.png)![page3](02192_页面_03.png)![page5](02192_页面_05.png)![page7](02192_页面_07.png) --&gt;
&lt;center&gt;
  









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_01.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_01_hu7b4e8c585c78596d52f33bba49723711_179891_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_页面_01.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_03.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_03_hu7b4e8c585c78596d52f33bba49723711_221609_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_页面_03.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_05.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_05_hu7b4e8c585c78596d52f33bba49723711_201925_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_页面_05.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-pages&#34; href=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_07.png&#34; &gt;
      &lt;img src=&#34;https://chengzhag.github.io/publication/im3d/pages/02192_%E9%A1%B5%E9%9D%A2_07_hu7b4e8c585c78596d52f33bba49723711_293071_0x190_resize_lanczos_2.png&#34; loading=&#34;lazy&#34; alt=&#34;02192_页面_07.png&#34; width=&#34;147&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;/center&gt;
&lt;center&gt;
  &lt;!-- &lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34;  class=&#34;btn btn-primary px-3 py-3&#34;&gt;arXiv&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;
 --&gt;
  &lt;!-- &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Paper&lt;/a&gt; --&gt;
  &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary js-cite-modal&#34; data-filename=&#34;/publication/im3d/cite.bib&#34;&gt;
  Cite
  &lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  arXiv
  &lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://arxiv.org/pdf/2103.06422&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Paper
  &lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;02192-supp.pdf&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Supp
  &lt;/a&gt;
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/pidan1231239/Implicit3DUnderstanding&#34; class=&#34;btn btn-outline-primary&#34; target=&#34;_blank&#34;&gt;
  Code
  &lt;/a&gt;
&lt;/center&gt;
&lt;!-- &lt;center&gt;
  &lt;a href=&#34;https://arxiv.org/abs/2103.06422&#34;&gt;[arXiv]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://arxiv.org/pdf/2103.06422&#34;&gt;[Paper]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;02192-supp.pdf&#34;&gt;[Supp]&lt;/a&gt; 
  &amp;nbsp; &amp;nbsp;
  &lt;a href=&#34;https://github.com/pidan1231239/Implicit3DUnderstanding&#34;&gt;[GitHub]&lt;/a&gt;
&lt;/center&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients&lt;/li&gt;
&lt;li&gt;And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding&lt;/li&gt;
&lt;li&gt;Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-pipeline-we-initialize-the-layout-estimation-and-3d-object-poses-with-len-and-odn-from-prior-work-then-refine-them-with-scene-graph-convolutional-network-sgcn-we-utilize-a-local-implicit-embedding-network-lien-to-encode-latent-code-for-ldif-decoder-and-to-extract-implicit-features-for-sgcn-with-the-help-of-ldif-and-marching-cube-algorithm-object-meshes-are-extracted-then-rotated-scaled-and-put-into-places-to-construct-the-scene&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.&#34; srcset=&#34;
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png 400w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_ef8517bd9f675e4bedaf968e44c025e5.png 760w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage.
In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry.
The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose.
In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-qualitative-comparison-on-object-detection-and-scene-reconstruction-we-compare-object-detection-results-with-total3d-and-ground-truth-in-both-oblique-view-and-camera-view-the-results-show-that-our-method-gives-more-accurate-bounding-box-estimation-and-with-less-intersection-we-compare-scene-reconstruction-results-with-total3d-in-camera-view-and-observe-more-reasonable-object-poses&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.&#34; srcset=&#34;
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png 400w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_74508ae60005474b267bfa284ca2971f.png 760w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png&#34;
               width=&#34;666&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Server System</title>
      <link>https://chengzhag.github.io/project/deep_earning_server_system/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/project/deep_earning_server_system/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For a small research team with more than a dozen of students, several servers with 4 or 8 GPUs are necessary. However, with independent storage for each servers, datasets, codes, and environments are often duplicated across different servers, which is inefficient and troublesome when using different servers. Also, user management becomes a problem if every user needs to be setup on each of the server.&lt;/p&gt;
&lt;p&gt;To solve the above problems, a system is built with centralized network storage, server and user management, 10 GbE network, and per-server SSD cache. This can be used as a reference for small academic teams on deep learning or maybe small companies.&lt;/p&gt;
&lt;p&gt;For new users and admins, &lt;a href=&#34;https://github.com/chengzhag/nas_directory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documents&lt;/a&gt; (in Chinese) are wrote to provide basic informations and regulations.&lt;/p&gt;
&lt;h2 id=&#34;hardware&#34;&gt;Hardware&lt;/h2&gt;
&lt;h3 id=&#34;servers-and-network&#34;&gt;Servers and Network&lt;/h3&gt;
&lt;p&gt;Our team has 5 existing servers, each with 4 x 1080Ti and a system SSD. The problem is that every server is not able to install an extra 10 GbE Network Interface Card (NIC) since all PCIE ports are preserved for GPUs. Thus an NBASE-T switch is used to connect each server via 5 GbE USB NICs. (A more stable and affordable configuration might be using 10 GbE SFP+ switch with PCIE NIC.)&lt;/p&gt;














&lt;figure  id=&#34;figure-no-space-to-install-10-gbe-nic-for-existing-server&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;No space to install 10 GbE NIC for existing server&#34; srcset=&#34;
               /project/deep_earning_server_system/%E7%85%A7%E7%89%87%20012_hu1aff075b4e8a5dba3f71e9ad4b16f525_247758_3de88a8d69dcaa38351698e7f733ed51.jpg 400w,
               /project/deep_earning_server_system/%E7%85%A7%E7%89%87%20012_hu1aff075b4e8a5dba3f71e9ad4b16f525_247758_228ef1ef075dd83911d6950962fa0b34.jpg 760w,
               /project/deep_earning_server_system/%E7%85%A7%E7%89%87%20012_hu1aff075b4e8a5dba3f71e9ad4b16f525_247758_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/project/deep_earning_server_system/%E7%85%A7%E7%89%87%20012_hu1aff075b4e8a5dba3f71e9ad4b16f525_247758_3de88a8d69dcaa38351698e7f733ed51.jpg&#34;
               width=&#34;30%&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      No space to install 10 GbE NIC for existing server
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;
&lt;p&gt;We chose to use QNAP Network Attached Storage (NAS) TS-932X, which can accommodate 5 x 3.5-inch hard drives and 4 x 2.5-inch SSDs, and has dual 10GbE SFP+ ports. The performance of its ARM processor is quit as SSD cache is configured on each server, which makes it possible to load frequently used data from NAS at the first time.&lt;/p&gt;














&lt;figure  id=&#34;figure-nas-and-switch&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;NAS and switch&#34; srcset=&#34;
               /project/deep_earning_server_system/IMG_20191107_161030_hub5951621c5a662dcd39dd207dc787f1e_169416_92b960d00a745cd90d293a8257142fd1.jpg 400w,
               /project/deep_earning_server_system/IMG_20191107_161030_hub5951621c5a662dcd39dd207dc787f1e_169416_af1397ccbb4cc064edbab70c06ca431b.jpg 760w,
               /project/deep_earning_server_system/IMG_20191107_161030_hub5951621c5a662dcd39dd207dc787f1e_169416_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://chengzhag.github.io/project/deep_earning_server_system/IMG_20191107_161030_hub5951621c5a662dcd39dd207dc787f1e_169416_92b960d00a745cd90d293a8257142fd1.jpg&#34;
               width=&#34;30%&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      NAS and switch
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;software&#34;&gt;Software&lt;/h2&gt;
&lt;h3 id=&#34;nas&#34;&gt;NAS&lt;/h3&gt;
&lt;p&gt;We configure our NAS with a &lt;code&gt;Public&lt;/code&gt; folder (contraining the git repo of &lt;a href=&#34;https://github.com/chengzhag/nas_directory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documents&lt;/a&gt;) for &lt;a href=&#34;https://github.com/chengzhag/nas_directory/tree/master/datasets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shared datasets&lt;/a&gt; storage, and a home folder for each user, allowing users to access their environment (e.g. anaconda), codes, and datasets on every server. QNAP NAS provides an easy-to-use operating system with serveral customizable services. In our cases, it is configured with the following services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NFS server: For mounting storage on server&lt;/li&gt;
&lt;li&gt;samba server: For mounting storage on client computers&lt;/li&gt;
&lt;li&gt;LDAP server: Centralized authorization and user management for both NAS and GPU server&lt;/li&gt;
&lt;li&gt;Other services like Download Station, file management from browser, restoring from snapshot/recycle bin, Qsync, VPN, DDNS, and iperf3 are also available to users.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;server&#34;&gt;Server&lt;/h3&gt;
&lt;p&gt;Each server is installed with same version of Ubuntu and Nvidia GPU driver, and is configured with the following services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;autofs: Automatically mount two types of folders (i.e. user home folders to &lt;code&gt;/home&lt;/code&gt; and &lt;code&gt;Public&lt;/code&gt; folder to &lt;code&gt;/media&lt;/code&gt;) through NFS&lt;/li&gt;
&lt;li&gt;cachefilesd: Local SSD cache for NFS&lt;/li&gt;
&lt;li&gt;LDAP authentication: Configured with Pluggable Authentication Modules (PAM)&lt;/li&gt;
&lt;li&gt;ThinLinc: Remote desktop&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the configurations above, students can use the same username to login to NAS or any server, access their data or share datasets with others, and configure anaconda environments once to use them on all servers. A &lt;a href=&#34;https://github.com/chengzhag/nas_directory/blob/master/documents/server/initserver.sh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shell script&lt;/a&gt; is used to configure servers. It is validated on Ubuntu 18 and might be used as a reference. For easier deployment and management of servers, virtual machine system (e.g. Proxmox) is installed on each server, with GPU passthrough configured. The difference between virtual machine and bare-metal Ubuntu is barely sensible.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
