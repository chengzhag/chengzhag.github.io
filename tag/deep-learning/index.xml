<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Cheng Zhang</title>
    <link>https://chengzhag.github.io/tag/deep-learning/</link>
      <atom:link href="https://chengzhag.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 11 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengzhag.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://chengzhag.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Holistic 3D Scene Understanding from a Single Image with Implicit Representation</title>
      <link>https://chengzhag.github.io/publication/im3d/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://chengzhag.github.io/publication/im3d/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
&lt;h2 id=&#34;3d-scene-understanding&#34;&gt;3D Scene Understanding&lt;/h2&gt;
&lt;p&gt;Given a single color image,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the room layout, including object categories and poses in 3D space&lt;/li&gt;
&lt;li&gt;Reconstruct mesh of individual object&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients&lt;/li&gt;
&lt;li&gt;And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding&lt;/li&gt;
&lt;li&gt;Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-proposed-pipeline-we-initialize-the-layout-estimation-and-3d-object-poses-with-len-and-odn-from-prior-work-then-refine-them-with-scene-graph-convolutional-network-sgcn-we-utilize-a-local-implicit-embedding-network-lien-to-encode-latent-code-for-ldif-decoder-and-to-extract-implicit-features-for-sgcn-with-the-help-of-ldif-and-marching-cube-algorithm-object-meshes-are-extracted-then-rotated-scaled-and-put-into-places-to-construct-the-scene&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.&#34; srcset=&#34;
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png 400w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_ef8517bd9f675e4bedaf968e44c025e5.png 760w,
               /publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/pipeline_hu5d54da59d234241780e2960c3ffaee78_1922141_973334ed13e4d2cb3eabdf9219279ebe.png&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage.
In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry.
The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose.
In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-qualitative-comparison-on-object-detection-and-scene-reconstruction-we-compare-object-detection-results-with-total3d-and-ground-truth-in-both-oblique-view-and-camera-view-the-results-show-that-our-method-gives-more-accurate-bounding-box-estimation-and-with-less-intersection-we-compare-scene-reconstruction-results-with-total3d-in-camera-view-and-observe-more-reasonable-object-poses&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.&#34; srcset=&#34;
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png 400w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_74508ae60005474b267bfa284ca2971f.png 760w,
               /publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://chengzhag.github.io/publication/im3d/results_hu40da7b1c92c60a32bded852532bb1b8e_2223271_974e0c0e3b6a4e4b9629f7b302cf9883.png&#34;
               width=&#34;666&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
  </channel>
</rss>
