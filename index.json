[{"authors":null,"categories":null,"content":"I am a Ph.D student co-supervised by Prof. Jianfei Cai, Dr. Camilo Cruz Gambardella and Prof. Dinh Phung at Monash University, Department of Data Science and AI. Before that, I received my Master\u0026rsquo;s degree in Information and Communication Engineering under the supervision of Prof. ShuaichengÂ LIU at University of Electronic Science and Technology of China. During my master study, I worked closely with Prof. Zhaopeng Cui and Dr. Yinda Zhang. I received my Bachelor\u0026rsquo;s degree at UESTC in 2019.\n","date":1766361600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1766361600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D student co-supervised by Prof. Jianfei Cai, Dr. Camilo Cruz Gambardella and Prof. Dinh Phung at Monash University, Department of Data Science and AI. Before that, I received my Master\u0026rsquo;s degree in Information and Communication Engineering under the supervision of Prof.","tags":null,"title":"Cheng Zhang","type":"authors"},{"authors":["Cheng Zhang","Boying Li","Meng Wei","Yan-Pei Cao","Camilo Cruz Gambardella","Dinh Phung","Jianfei Cai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- arXiv 2026 Cheng Zhang 1,2 \u0026nbsp; \u0026nbsp; Boying Li 1 \u0026nbsp; \u0026nbsp; Meng Wei 1 Yan-Pei Cao 3 \u0026nbsp; \u0026nbsp; Camilo Cruz Gambardella 1,2 \u0026nbsp; \u0026nbsp; Dinh Phung 1 \u0026nbsp; \u0026nbsp; Jianfei Cai 1  1 Monash University \u0026nbsp; \u0026nbsp; 2 Building 4.0 CRC \u0026nbsp; \u0026nbsp; 3 VAST   Cite  -- Code  Video  bilibili  -- arXiv  Paper    Video   Our UCPE introduces a geometry-consistent alternative to PlÃ¼cker rays as one of the core contributions, enabling better generalization in Transformers. We hope to inspire future research on camera-aware architectures.\n TLDR ðŸ”¥ Camera-controlled text-to-video generation, now with intrinsics, distortion and orientation control!\nðŸ“· UCPE integrates Relative Ray Encodingâ€”which delivers significantly better generalization than PlÃ¼cker across diverse camera motion, intrinsics and lens distortionsâ€”with Absolute Orientation Encoding for controllable pitch and roll, enabling a unified camera representation for Transformers and state-of-the-art camera-controlled video generation with just 0.5% extra parameters (35.5M over the 7.3B parameters of the base model)\n Highlights Our Relative Ray Encoding not only generalizes to but also enables controllability over a wide range of camera intrinsics and lens distortions.\nIts geometry-consistent design further allows strong generalization and controllability over diverse camera motions.\nWe also introduce Absolute Orientation Encoding to eliminate the ambiguity in pitch and roll in previous T2V methods.\n Abstract Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks.\n Method   Spherical Camera Optical Flow. The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion f, the camera rotation yields an analytic rotation flow fr on the sphere. By decomposing f into fr and its residual, we obtain a derotated flow fd that more clearly captures camera translation and object dynamics.    Overview of Spatial Attention Adapter. The adapter injects UCPE into pretrained Transformers through a lightweight branch that preserves pretrained priors. It constructs hybrid encoding from the world-to-ray transform T^rw and an optional Lat-Up map, applies them within attention, and fuses the resulting camera-aware tokens back through a zero-initialized linear layer.  ","date":1766361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1766361600,"objectID":"29bc2ad7d1f10c32f3940305a26b5713","permalink":"https://chengzhag.github.io/publication/ucpe/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ucpe/","section":"publication","summary":"Camera-controlled text-to-video generation, now with intrinsics, distortion and orientation control!","tags":["Deep Learning","Video Generation","Camera Control","Camera Positional Encoding"],"title":"Unified Camera Positional Encoding for Controlled Video Generation","type":"publication"},{"authors":["Cheng Zhang","Hanwen Liang","Donny Y. Chen","Qianyi Wu","Konstantinos N. Plataniotis","Camilo Cruz Gambardella","Jianfei Cai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- AAAI 2026 Cheng Zhang 1,2 \u0026nbsp; \u0026nbsp; Hanwen Liang 3 \u0026nbsp; \u0026nbsp; Donny Y. Chen 1 \u0026nbsp; \u0026nbsp; Qianyi Wu 1 Konstantinos N. Plataniotis 3 \u0026nbsp; \u0026nbsp; Camilo Cruz Gambardella 1,2 \u0026nbsp; \u0026nbsp; Jianfei Cai 1  1 Monash University \u0026nbsp; \u0026nbsp; 2 Building 4.0 CRC \u0026nbsp; \u0026nbsp; 3 University of Toronto   Cite  -- Code  Video  bilibili  -- arXiv  Paper    Video    Abstract Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence.\n Method   Spherical Camera Optical Flow. The optical flow from a panoramic video (left) can be interpreted as a spherical camera optical flow (right). For complex motion f, the camera rotation yields an analytic rotation flow fr on the sphere. By decomposing f into fr and its residual, we obtain a derotated flow fd that more clearly captures camera translation and object dynamics.    Our proposed PanFlow pipeline. Given an input image and text prompt, PanFlow uses a decoupled motion from a video as reference to generate a panoramic video. We first estimate a decoupled optical flow from the reference video, of which the derotated flow is used to generate a latent noise with spherical noise warping. The latent noise then serves as a motion condition for a video diffusion transformer with LoRA fine-tuning to generate derotated videos. Finally, the decoupled rotation is accumulated and applied to the generated video frames to recover the full motion.   Applications   -- By conditioning diffusion on spherical-warped motion noise, PanFlow enables precise motion control, produces loop-consistent panoramas, and supports applications such as motion transfer:\nand panoramic video editing:\n","date":1766275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1766275200,"objectID":"b32004b53ac55339b8f405b3b79896d2","permalink":"https://chengzhag.github.io/publication/panflow/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/panflow/","section":"publication","summary":"PanFlow is a framework for controllable 360Â° panoramic video generation that decouples motion input into two interpretable components: rotation flow and derotated flow.","tags":["Deep Learning","Video Generation","Panorama","Motion Control"],"title":"PanFlow: Decoupled Motion Control for Panoramic Video Generation","type":"publication"},{"authors":["Cheng Zhang","Haofei Xu","Qianyi Wu","Camilo Cruz Gambardella","Dinh Phung","Jianfei Cai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- CVPR 2025 Cheng Zhang 1,2 \u0026nbsp; \u0026nbsp; Haofei Xu 3 \u0026nbsp; \u0026nbsp; Qianyi Wu 1 Camilo Cruz Gambardella 1,2 \u0026nbsp; \u0026nbsp; Dinh Phung 1 \u0026nbsp; \u0026nbsp; Jianfei Cai 1  1 Monash University \u0026nbsp; \u0026nbsp; 2 Building 4.0 CRC, Caulfield East, Victoria, Australia \u0026nbsp; \u0026nbsp; 3 ETH Zurich   Cite  -- Code  YouTube  bilibili  -- arXiv  Paper    Short Video    Abstract With the advent of portable 360Â° cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 Ã— 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 Ã— 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets.\n Full Video    Pipeline   Our proposed PanSplat pipeline. Given two wide-baseline panoramas, we first construct a hierarchical spherical cost volume using a Transformer-based FPN to extract feature pyramid and 2D U-Nets to integrate monocular depth priors for cost volume refinement. We then build Gaussian heads to generate a feature pyramid, which is later sampled with Fibonacci lattice and transformed to spherical 3D Gaussian pyramid. Finally, we unproject the Gaussian parameters for each level and view, consolidate them into a global representation, and splat it into novel views using a cubemap renderer. For simplicity, intermediate results of only a single view are shown.   Interactive Demo    (Best viewed on a desktop browser or Youtube app.)\n Results   ","date":1733961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733961600,"objectID":"66418d7fe2db1f9ef6027a449a111cee","permalink":"https://chengzhag.github.io/publication/pansplat/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pansplat/","section":"publication","summary":"We present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 Ã— 4096).","tags":["Deep Learning","Novel View Synthesis","Panorama"],"title":"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting","type":"publication"},{"authors":["Cheng Zhang","Qianyi Wu","Camilo Cruz Gambardella","Xiaoshui Huang","Dinh Phung","Wanli Ouyang","Jianfei Cai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- CVPR 2024 Highlight Cheng Zhang 1,3 \u0026nbsp; \u0026nbsp; Qianyi Wu 1 \u0026nbsp; \u0026nbsp; Camilo Cruz Gambardella 1,3 \u0026nbsp; \u0026nbsp; Xiaoshui Huang 2 Dinh Phung 1 \u0026nbsp; \u0026nbsp; Wanli Ouyang 2 \u0026nbsp; \u0026nbsp; Jianfei Cai 1  1 Monash University \u0026nbsp; \u0026nbsp; 2 Shanghai AI Laboratory \u0026nbsp; \u0026nbsp; 3 Building 4.0 CRC, Caulfield East, Victoria, Australia   Cite  Code  YouTube  bilibili  -- arXiv  Paper      Abstract Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at https://chengzhag.github.io/publication/panfusion.\n Pipeline   Our proposed dual-branch PanFusion pipeline. The panorama branch (upper) provides global layout guidance and registers the perspective information to get seamless panorama output. The perspective branch (lower) harnesses the rich prior knowledge of Stable Diffusion (SD) and provides guidance to alleviate distortion under perspective projection. Both branches employ the same UNet backbone with shared weights, while finetuned with separate LoRA layers. Equirectangular-Perspective Projection Attention (EPPA) modules are plugged into different layers of the UNet to pass information between the two branches.   Comparisons               Generalization        ","date":1712793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712793600,"objectID":"9e21dceb9587391aa0c58ef5c2cc6f59","permalink":"https://chengzhag.github.io/publication/panfusion/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/panfusion/","section":"publication","summary":"We introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt.","tags":["Deep Learning","Image Generation","Panorama"],"title":"Taming Stable Diffusion for Text to 360Â° Panorama Image Generation","type":"publication"},{"authors":null,"categories":["Photoblog"],"content":"The photo was taken from the plane to Australia at a height of 11,000 meters. This is my first trip to another country.\n  Nikon D3300. f/4.8, 13mm, 30s, ISO 6400.  ","date":1687910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687910400,"objectID":"2f0bff54b767c54c5682e777c31c13e1","permalink":"https://chengzhag.github.io/post/trip_to_australia/","publishdate":"2023-06-28T00:00:00Z","relpermalink":"/post/trip_to_australia/","section":"post","summary":"A photo of the milky way taken on the plane to Australia.","tags":["Photo"],"title":"Trip to Australia","type":"post"},{"authors":["Cheng Zhang","Zhaopeng Cui","Cai Chen","Shuaicheng Liu","Bing Zeng","Hujun Bao","Yinda Zhang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ICCV 2021 Oral Cheng Zhang 1 \u0026nbsp; \u0026nbsp; Zhaopeng Cui 2 \u0026nbsp; \u0026nbsp; Cai Chen 1 \u0026nbsp; \u0026nbsp; Shuaicheng Liu 1 \u0026nbsp; \u0026nbsp; Bing Zeng 1 \u0026nbsp; \u0026nbsp; Hujun Bao 2 \u0026nbsp; \u0026nbsp; Yinda Zhang 3  1 University of Electronic Science and Technology of China 2 State Key Lab of CAD \u0026 CG, Zhejiang University \u0026nbsp; \u0026nbsp; 3 Google     Abstract Panorama images have a much larger field-of-view thus naturally encode enriched scene context information compared to standard perspective images, which however is not well exploited in the previous scene understanding methods. In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room layout and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to predict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geometry accuracy and object arrangement.\n Video    Paper         Cite  Code  YouTube  bilibili  arXiv (Supp included)  Paper    Pipeline     Our proposed pipeline. We first do a bottom-up initialization with several SoTA methods and provide various features, including geometric, semantic, and appearance features of objects and layout. These are then fed into our proposed RGCN network to refine the initial object pose and estimate the relation among objects and layout. A relation optimization is adopted afterward to further adjust the 3d object arrangement to align with the 2D observation, conform with the predicted relation, and resolve physical collision.   We first extract the whole-room layout under Manhattan World assumption and the initial object estimates including locations, sizes, poses, semantic categories, and latent shape codes. These, along with extracted features, are then fed into the Relation-based Graph Convolutional Network (RGCN) for refinement and to estimate relations among objects and layout simultaneously. Then, a differentiable Relation Optimization (RO) based on physical violation, observation, and relation is proposed to resolve collisions and adjust object poses. Finally, the 3D shape is recovered by feeding the latent shape code into Local Implicit Deep Function (LDIF), and combined with object pose and room layout to achieve total scene understanding.\n Interactive Results   Input\n Detection and Layout\n Reconstruction\n                                                                 Results   Qualitative comparison on 3D object detection and scene reconstruction. We compare object detection and compare scene reconstruction results with Total3D-Pers and Im3D-Pers in both bird\u0026rsquo;s eye view and panorama format.  ","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072000,"objectID":"13179e3833c482c7b6079c40f3ea1097","permalink":"https://chengzhag.github.io/publication/dpc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dpc/","section":"publication","summary":"We propose a novel method for panoramic 3D scene understanding which recovers the 3D room layout and the shape, pose, position, and semantic category for each object from a single full-view panorama image.","tags":["Deep Learning","3D Reconstruction","3D Detection","3D Scene Understanding","Panorama"],"title":"DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization","type":"publication"},{"authors":["Cheng Zhang","Zhaopeng Cui","Yinda Zhang","Bing Zeng","Marc Pollefeys","Shuaicheng Liu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- CVPR 2021 Cheng Zhang 2* \u0026nbsp; \u0026nbsp; Zhaopeng Cui 1* \u0026nbsp; \u0026nbsp; Yinda Zhang 3* \u0026nbsp; \u0026nbsp; Bing Zeng 2 \u0026nbsp; \u0026nbsp; Marc Pollefeys 4 \u0026nbsp; \u0026nbsp; Shuaicheng Liu 2  1 State Key Lab of CAD \u0026 CG, Zhejiang University 2 University of Electronic Science and Technology of China \u0026nbsp; \u0026nbsp; 3 Google \u0026nbsp; \u0026nbsp; 4 ETH Zurich     Abstract We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.\n Video    Paper          arXiv   -- Paper -- Cite  Code  YouTube  bilibili  arXiv  Paper   [arXiv] \u0026nbsp; \u0026nbsp; [Paper] \u0026nbsp; \u0026nbsp; [Supp] \u0026nbsp; \u0026nbsp; [GitHub]  --  Motivations  Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation   Pipeline   Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.  The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage. In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry. The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose. In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.\n Interactive Results   Input\n Total3D\n Ours\n                                                                       Results   Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.  ","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"83623508ea0f4df7d9acb4b00885826b","permalink":"https://chengzhag.github.io/publication/im3d/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/im3d/","section":"publication","summary":"We present a new pipeline which takes a single image as input, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation.","tags":["Deep Learning","3D Reconstruction","3D Detection","3D Scene Understanding","Layout Estimation"],"title":"Holistic 3D Scene Understanding from a Single Image with Implicit Representation","type":"publication"},{"authors":null,"categories":null,"content":" Introduction For a small research team with more than a dozen of students, several servers with 4 or 8 GPUs are necessary. However, with independent storage for each servers, datasets, codes, and environments are often duplicated across different servers, which is inefficient and troublesome when using different servers. Also, user management becomes a problem if every user needs to be setup on each of the server.\nTo solve the above problems, a system is built with centralized network storage, server and user management, 10 GbE network, and per-server SSD cache. This can be used as a reference for small academic teams on deep learning or maybe small companies.\nFor new users and admins, documentation (in Chinese) are wrote to provide basic informations and regulations.\nHardware Servers and Network Our team has 5 existing servers, each with 4 x 1080Ti and a system SSD. The problem is that every server is not able to install an extra 10 GbE Network Interface Card (NIC) since all PCIE ports are preserved for GPUs. Thus an NBASE-T switch is used to connect each server via 5 GbE USB NICs. (A more stable and affordable configuration might be using 10 GbE SFP+ switch with PCIE NIC.)\n  No space to install 10 GbE NIC for existing server  Storage We chose to use QNAP Network Attached Storage (NAS) TS-932X, which can accommodate 5 x 3.5-inch hard drives and 4 x 2.5-inch SSDs, and has dual 10GbE SFP+ ports. The performance of its ARM processor is quit as SSD cache is configured on each server, which makes it possible to load frequently used data from NAS at the first time.\n  NAS and switch  Software NAS We configure our NAS with a Public folder (contraining the git repo of documentation) for shared datasets storage, and a home folder for each user, allowing users to access their environment (e.g. anaconda), codes, and datasets on every server. QNAP NAS provides an easy-to-use operating system with serveral customizable services. In our cases, it is configured with the following services:\n NFS server: For mounting storage on server samba server: For mounting storage on client computers LDAP server: Centralized authorization and user management for both NAS and GPU server Other services like Download Station, file management from browser, restoring from snapshot/recycle bin, Qsync, VPN, DDNS, and iperf3 are also available to users.  Server Each server is installed with same version of Ubuntu and Nvidia GPU driver, and is configured with the following services:\n autofs: Automatically mount two types of folders (i.e. user home folders to /home and Public folder to /media) through NFS cachefilesd: Local SSD cache for NFS LDAP authentication: Configured with Pluggable Authentication Modules (PAM) ThinLinc: Remote desktop  With the configurations above, students can use the same username to login to NAS or any server, access their data or share datasets with others, and configure anaconda environments once to use them on all servers. A shell script is used to configure servers. It is validated on Ubuntu 18 and might be used as a reference. For easier deployment and management of servers, virtual machine system (e.g. Proxmox) is installed on each server, with GPU passthrough configured. The difference between virtual machine and bare-metal Ubuntu is barely sensible.\n","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"b9bcc7ff90adaaa3fbb8f29daff8aa1e","permalink":"https://chengzhag.github.io/project/deep_earning_server_system/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/project/deep_earning_server_system/","section":"project","summary":"A deep learning system for small research teams, with multiple GPU servers, centralized network storage, server and user management, 10 GbE network, and per-server SSD cache.","tags":["Deep Learning"],"title":"Deep Learning Server System","type":"project"},{"authors":null,"categories":["Photoblog"],"content":"The photo is taken from an airplane when preparing to land in Chengdu. The conditions and timing for taking this photo were demanding. First, the city lights in the backgound is only visible in a full left turn. Second, measures must be taken to avoid the shaking of hand and the reflection on the window.\n  Nikon D3300. f/5.6, 20mm, 5s, ISO 1600.  ","date":1566000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566000000,"objectID":"ebc4af732736ca6c7dfe5aa2c6426470","permalink":"https://chengzhag.github.io/post/plane_turn_long_exposure/","publishdate":"2019-08-17T00:00:00Z","relpermalink":"/post/plane_turn_long_exposure/","section":"post","summary":"The conditions and timing for taking this photo were demanding. First, the city lights in the backgound is only visible in a full left turn. Second, measures must be taken to avoid the shaking of hand and the reflection on the window.","tags":["Photo"],"title":"Long Exposure of Airplane Turning","type":"post"},{"authors":null,"categories":null,"content":" Introduction This project is a work of team of 3, which includes hsuhaoo and Zijing Guan. We were luckily supported and guided by Prof. Jie Zhuang. The project can be further extended to applications like fall detection for the elderly.\nThe radar works in the 2.7 GHz - 3.7 GHz band, and has 4 transmitting antennas and 12 receive antennas. With the power of less than 10 dBm, it can measure the distance of a person with a median of 5 cm. Within 5 meters, it has an positioning accuracy of about 20 cm. Without further tests, the maximum distance is unknown. However, it can indicate the existance of a person behind a 25 cm concrete wall.\nCode and documents are available here. Slides introducing hardware and data processing process (in Chinese) are available here.\nFMCW  Frequency-modulated continuous-wave radar (FM-CW) â€“ also called continuous-wave frequency-modulated (CWFM) radar â€“ is a short-range measuring radar set capable of determining distance (Continuous-wave radar wiki).\n If you have no idea about FMCW, the following tutorials can be a good starting point.\n Frequency-Modulated Continuous-Wave Radar (FMCW Radar): a brief introduction of the principle of FMCW Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging: an MIT course to build an FMCW radar yourself GUEST POST: TRY RADAR FOR YOUR NEXT PROJECT: an article written by the instructor of the course above Intro to mmWave Sensing: FMCW Radars - Module 1: Range Estimation: a series of 5 short videos providing a concise yet in-depth introduction to sensing using FMCW radars  SDR  Software-defined radio (SDR) is a radio communication system where components that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system.\n We use a USRP N210 with an LFRX daughter board to sample the baseband signal. The RF front end is made with RF modules and the FMCW generator is build on a VCO. The USRP here acts as an ADC and is connected to Simulink. Following links show how to connect a USRP to Simulink:\n USRPÂ® Support Package from Communications System Toolbox: Matlab toolbox for USRP Digital Communication Systems Engineering Using Software Defined Radio: a tutorial of SDR using Simulink   Hardware In this section, we introduce the hardware design in two parts, i.e. RF front end and FMCW generator. We refer to the following projects and papers for basic ideas:\n Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging RF-Capture: paper see RF-Capture: Capturing a Coarse Human Figure Through a Wall WiTrack: papers see 3D Tracking via Body Radio Reflections, Multi-Person Localization via RF Body Reflections  FMCW generator Hardware:\n ADF4159 evaluation board: EV-ADF4159EB3Z Loop filter: AD8065. It is designed with ADIsimPLL under the guidance of CN-0302. VCO: ZX95-3800A+ Power splitter: ZX10-2-42+ Attenuator: VAT-3+  Our FMCW generator works in 2.7GHz-3.7GHz band to avoid 2.4GHz wifi signal.\n  FMCW generator  RF front end Hardware:\n PA and LNA: ZX60-53LNB+ Power splitter: ZN2PD-9G+ Attenuator: FW-9+ Mixer: ZX05-43+ Switch: ADRF5040 mbed board: ST NUCLEO-L476RG  The mbed board detects the trigger edge and add a digital signal after the edge to indicate id of antenna pair. The baseband signal from the mixer and the sync signal from mbed board are first sampled with the LFRX daughter board simultaneously, then analyzed with Simulink in real time.\n  RF front end   Software In this section, we introduce software design in three parts, i.e. ADF4159 evaluation board configuration, mbed code, and Simulink model.\nADF4159 The ADF4159 evaluation board is configured using ADF4158 and ADF4159 Evaluation Board Software via USB. The configuration file is available here. The board is configured with follow parameters:\n Ramp mode: Continuous sawtooth RF frequency: 2.7 GHz - 3.7 GHz Ramp frequency: 2000 Hz Charge pump: 1.25 mA Muxout: Digital lock Detection. It is used as a trigger to switch antenna pairs and a sync signal to align base band signals of each antenna pair. The muxout port outputs a falling edge on the start of a ramp.  The 4 Tx and 12 Rx antennas makes up 4*12 antenna pairs. With ramp frequency of 2000 Hz, every antenna pair can be activated for 41.7 times per second.\nmbed code The mbed board switches antenna pairs after every trigger edge from the ADF4159 Muxout pin. And as mentioned before, it adds a digital signal after the edge to indicate id of antenna pair. Considering that MCU is running at a relatively low frequency, which makes the responce to the sync signal unstable, we design the digital signal from MCU to pull down the sync signal after the trigger edge. In this way, the falling edge is kept synced. Code of mbed board is available here.\n  Antenna switching timing diagram  Simulink model All the models are in the simulink folder. RadarImagingAndPositioning.slx block process baseband signals from USRP and output the 2D heat map and target position. The data processing pipeline is shown in the slides mentioned before. usrp_4t12r_heatmap.slx cooperates the above block with others to show the imaging and target detection results.\n Results In the experimental stage, A system with only 1 Tx and 3 Rx is built for experimental usage.\n  1TX3RX system  The sync signal is shown below.\n  Sync signal  Waterfall plot of each antenna pair is shown below. A person is walking away then back in this experiment. The d axis shows the round trip of the radar signal.\n  waterfall plot of each antenna pair  The system is then expanded to 1 Tx and 8 Rx, which allows 2D imaging with phased array algorithm.\n  1TX8RX system  The image bellow shows the heat map captured from the 1Tx8Rx system. The x-axis indicates angle from left to right. The y-axis indicates the round distance of the radar signal. A person is standing in front of the radar at a distance of about 4 meters in this case.\n  1Tx8Rx top-down heatmap  The system is further expanded to 4 Tx and 12 Rx, which allows 3D imaging.\n  4TX12RX system  The following GIF is the 3D imaging projected to the front view, showing a person drawing a circle in the air with a corner reflector.\n Front view heatmap  The GIF below shows the 2D imaging from above with target detection activated. A person is walking around in the distance of about 3 meters. The y-axis indicates the round distance of radar signal.\n Top-down view heatmap  We tried to extract the signal strength vertical axis of person target doing squatting in the 10th second. However, as the radio waves in this frequency acts more like mirror reflection on the human body, the angle of the reflection surface becomes a issue as mentioned in RF-Capture. This makes the height of a person unpredictable for our system.\nThe figure bellow shows the strength of the reflected signal in the vertical axis vs time. The heat map indicates that the height of the person drops several times during the first 10 seconds, while the target only moves slightly with no changes on height.\n  Height axis heatmap vs time  The final work was put together on a cart.\n  Final system  ","date":1527465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527465600,"objectID":"c6fa4c6a2d34bff5f689074f080f31dd","permalink":"https://chengzhag.github.io/project/fmcw_positioning_radar/","publishdate":"2018-05-28T00:00:00Z","relpermalink":"/project/fmcw_positioning_radar/","section":"project","summary":"A FMCW Radar with 4 transmitting antennas and 12 receive antennas working within the 2.7 GHz - 3.7 GHz band that can locate a person in the same room with accuracy of 20 cm.","tags":["Radar","Contest","Electronics Design"],"title":"FMCW Positioning Radar","type":"project"},{"authors":null,"categories":null,"content":" Introduction This project is a work of team of 3, which includes shicaiwei123 and zianglei. The preparations of the contest are done with the help of yoyolalala. I was responsible for building mechanical structures and embedded programming (i.e. STM32, Raspberry Pi). We are lucky to have received the first prize in 2017NUEDC. The final work has been handed in to our school thus details will not be provided. However, code and document can be found here.\n The ball-on-plate system is a promoted version of the traditional ball-on-beam control problem. The problem consists of a plate which its deviation can be manipulated in two perpendicular directions. The goal is to carry the ball moving on the plate to a desired position, that is to control a freely rolling ball on a specific position or moving on a trajectory on the plate. - Modelling and Control of Ball-Plate System   Ball-on-plate system \n Specifically in this project, according to the contest requirements, the system is supposed to move a ball between any two of nine evenly distributed circles and avoids the others. The plate must have a size around 65cm*65cm. The document shows the distribution of the circles.\n The Contest NUEDC is one of the largest electronic design contest for undergraduate students in China. The contest lasts three and a half days. More than ten topics of four types including control, measurement, communication and power are published in the first day morning, indicating the start of the contest. Participants usually prepare for a long time before the contest. We attended the contest in 2017 starting from August 9th to 12th. Our team started the preparation four months in advance.\nAlthough I had got some experience with programming, I barely knew the basics of control system. However, I chose this type of topic just in the interest of it.\nIn the preparation process, we learned from five projects, three of which are listed below.\n Rotary Inverted Pendulum Ball on Beam Ball on Small Plate  Sadly, no video demo was taken for the limit of time.\n Platform  Raspberry Pi Zero and Raspberry Pi 3b with OpenCV installed STM32F103 minimum system board PC with Visual Studio and VisualGDB installed  The Raspberry Pi is developed with C++ using Raspberry Pi toolchain provided by VisualGDB (tutorial here). It deals with the machine vision tasks and sends the results to STM32 through UART.\nThe STM32 is developed with C++ language using Arm toolchain provided by VisualGDB (tutorial here). A C++ API for STM32 (Ebox) was used as a replacement of ardunio, which is not allowed in the contest.\n Mechanical structure The final work uses a PCB motherboard (designed by shicaiwei123) same as Ball on Small Plate project to connect modular electronic parts.\n  PCB mother board  The 65cm*65cm plate is made of laminated balsa wood board.\n  The plate  We design metre-shaped ribs to further reinforce the plate. We follow Ball on small plate to support the plate with a universal joint in the center and drive the board with two servos using ball joint rods as connection.\n  Under the plate  Design documents of the reinforcing ribs are also available here.\n  Reinforcing ribs    Cardan joint    Servo with connecting rod  The camera is supported by carbon fiber tubes like in Ball on small plate. And the same ring light is installed around the camera to provide better illumination.\n  LED around camera  The Raspberry Pi 3b is hanged beyond the camera to reduce the load on the servos.\n  The complete work  The final work is packed with cardboard then submitted to the competition organizer.\n  Complete work packed with cardboard, kinda like a house   Final test After a successful test in front of the judges of Sichuan division, follwed by a basic on-site electronic design test, we went to Xi\u0026rsquo;an for the final test. Below shows some photos taken along the trip.\n      ","date":1504396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504396800,"objectID":"fedfb20833290a8d7531050fb8d70ca1","permalink":"https://chengzhag.github.io/project/ball_on_big_plate/","publishdate":"2017-09-03T00:00:00Z","relpermalink":"/project/ball_on_big_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 65cm wide plate, and is a work of 3 team-mates which get us first prize in 2017NUEDC.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Big Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Small Plate is built with a Raspberry Zero, a camera, two servos, a STM32F103 board, and is capable of setting target coordinate, circling around the center with adjustable radius. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details are discussed here.\n Demo    Mechanical structure First version The 20cm*20cm plate is made of 1mm black glass fiber board. In the first version shown in the video, the plate is covered with white paper to provide more contrast between ball and plate. Below shows the original plate.\n  Deisigning the mechanical structure  The plate is supported with a universal joint in the center and has 2 degrees of freedom. The angle of the plate is controled by two servos with ball joint rods connecting the servos and the plate.\n  Installation of plate and two servos  In this first version. The camera is fixed to the base, which requires extra procedure to segment the plate from background and warp back to squre. This introduces noise to the positioning of the ball, which is the major reason for the jittering in the video.\n  Installation of camera and Raspberry Pi Zero  Second version By fixing the camera to the plate with carbon fiber tubes, we managed to avoid the noise. The Raspberry Pi 3b is hanged beyond the camera to reduce the load on the servos. The plate itself is also changed to yellow thus no paper is needed. This reduces friction between the ball and the plate.\n  Camera is connected to the plate to reduce jitter  A ring light is installed around the camera for better illumination.\n  Camera and LED installation  Below shows the PCB board installed on the base, which is later used in project Ball on Big Plate.\n  PCB mother board  ","date":1496534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496534400,"objectID":"a849425d11bc301c05720583165b2c0d","permalink":"https://chengzhag.github.io/project/ball_on_small_plate/","publishdate":"2017-06-04T00:00:00Z","relpermalink":"/project/ball_on_small_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 20cm wide plate, and is capable of setting target coordinate, circle around the center with adjustable radius.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Small Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Beam is built with a Raspberry Zero, a camera, a servo, and a STM32F103 board. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details are discussed here.\n Demo   ","date":1495929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495929600,"objectID":"052b23df82dcd5f3f66e60e43c26940f","permalink":"https://chengzhag.github.io/project/ball_on_beam/","publishdate":"2017-05-28T00:00:00Z","relpermalink":"/project/ball_on_beam/","section":"project","summary":"Balancing a ball on a beam with a Raspberry Zero, a camera, one servo, a STM32F103 board.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Beam","type":"project"},{"authors":null,"categories":null,"content":" Introduction Rotary Inverted Pendulum is built with an encoder, a brush motor, a STM32F103 board, and has no rotation limit. It is capable of setting target angle, non-stop swinging and automatically swinging up. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details Code and documents are available here.\n Demo    Mechanical structure An optical encoder is used to measure the angle of the pendulum. Thus calibration needs to be down by relaxing the pendulum to get the absolute angle every time after reset.\n  Optical encoder  A slip ring (or collector ring) is used to prevent the winding of the wire.\n  Slip ring  The DC brush motor with reduction gearbox and magnetic encoder is driven by a TB6612FNG model controlled controled with PWM signal. A 3-ceils lithium battery is used as power.\n  Complete circuit board  The mechanical structure consists of three round plates which supports the slip ring, motor, and a quick-release plate. The round plates are connected with hexagonal copper pillars. The whole structure is mounted to a tripod through the quick-release plate.\n  The core mechanical structure    The complete mechanical structure   Software design  Please refer to Study on PID Control of a Single Inverted Pendulum System for the PID control system.   PID control system \n  Also we refer to Research and Implementation of the Swing - up and Stabilizing Operation for Rotational Inverted Pendulum for the swing-up and stabilizing operation.   Swing - up and stabilizing operation \n The swing-up and stabilizing operation and the changes between different modes are modeled as a state machine.\n  State machine for swing-up and stabilizing operation  ","date":1491696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491696000,"objectID":"a8f9d12f6d2d712d537277742928f936","permalink":"https://chengzhag.github.io/project/rotary_inverted_pendulum/","publishdate":"2017-04-09T00:00:00Z","relpermalink":"/project/rotary_inverted_pendulum/","section":"project","summary":"A rotary inverted pendulum system built with an encoder, a brush motor, a STM32F103 board that is capable of setting target orientation, non-stop swinging and automatically swinging up.","tags":["Contest","Control System","Electronics Design"],"title":"Rotary Inverted Pendulum","type":"project"},{"authors":null,"categories":["Photoblog"],"content":"Both campuses of University of Electronic Science and Technology of China (UESTC) in Chengdu are famous for their ginkgo. When autumn is in full swing, there is a constant stream of visitors. The scenes are captured as polar panoramas to show the full view of which.\n  Shahe Campus: No.4, Section 2, North Jianshe Road, Chengdu, Sichuan, China    Qingshuihe Campus: No.2006, Xiyuan Ave, West Hi-Tech Zone, Chengdu, Sichuan, China  ","date":1448755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448755200,"objectID":"41a65b93cd5ea0439f91a36ff2733da4","permalink":"https://chengzhag.github.io/post/ginkgo_panorama/","publishdate":"2015-11-29T00:00:00Z","relpermalink":"/post/ginkgo_panorama/","section":"post","summary":"Both campuses of UESTC in Chengdu are famous for their ginkgo. When autumn is in full swing, there is a constant stream of visitors.","tags":["Photo"],"title":"Ginkgo of UESTC","type":"post"},{"authors":null,"categories":["Photoblog"],"content":"In Milky Way Photography, Japanese photography Masahiro Miyasaka created a method to capture close-range objects (e.g. flowers) and stars far away in a single shot. With an automatic aperture lens, I managed to do the same while keeping the gorgeous bokeh by changing the focus distance instead of aperture size during the exposure time. After changing the focus distance, the flowers were illuminated with phone screen.\n  Nikon D3300. f/3.5, 18mm, 30s, ISO 3200.  ","date":1436745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436745600,"objectID":"e1beed3c22fbcdd82c8d521aee163e25","permalink":"https://chengzhag.github.io/post/qinhai_lake_milky_way/","publishdate":"2015-07-13T00:00:00Z","relpermalink":"/post/qinhai_lake_milky_way/","section":"post","summary":"In Milky Way Photography, Japanese photography Masahiro Miyasaka created a method to capture close-range objects (e.g. flowers) and stars far away in a single shot.","tags":["Photo"],"title":"Milky Way at Qinhai Lake","type":"post"}]