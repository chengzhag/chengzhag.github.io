[{"authors":null,"categories":null,"content":"I am a second-year master‘s student of machine learning and computer vision under the supervision of Shuaicheng LIU at University of Electronic Science and Technology of China. My research interests include 3D reconstruction, 3D detection, 3D scene understanding, etc. I received my Bachelor\u0026rsquo;s degree at UESTC in 2019.\n","date":1615420800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615420800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a second-year master‘s student of machine learning and computer vision under the supervision of Shuaicheng LIU at University of Electronic Science and Technology of China. My research interests include 3D reconstruction, 3D detection, 3D scene understanding, etc.","tags":null,"title":"Cheng Zhang","type":"authors"},{"authors":["Cheng Zhang","Zhaopeng Cui","Yinda Zhang","Bing Zeng","Marc Pollefeys","Shuaicheng Liu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --  Abstract We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.\n Paper          arXiv   -- Paper -- Cite  \u0026nbsp; \u0026nbsp; arXiv  \u0026nbsp; \u0026nbsp; Paper  \u0026nbsp; \u0026nbsp; Supp  \u0026nbsp; \u0026nbsp; GitHub   [arXiv] \u0026nbsp; \u0026nbsp; [Paper] \u0026nbsp; \u0026nbsp; [Supp] \u0026nbsp; \u0026nbsp; [GitHub]  --  Motivations  Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation   Pipeline   Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.  The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage. In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry. The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose. In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.\n Results   Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.  ","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"83623508ea0f4df7d9acb4b00885826b","permalink":"https://chengzhag.github.io/publication/im3d/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/im3d/","section":"publication","summary":"We present a new pipeline which takes a single image as input, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation.","tags":["Deep Learning","3D Reconstruction","3D Detection","3D Scene Understanding","Layout Estimation"],"title":"Holistic 3D Scene Understanding from a Single Image with Implicit Representation","type":"publication"},{"authors":null,"categories":null,"content":" Introduction FMCW Positioning Radar project is a work of team made up of three people. We were trying to get the qualification to attend ESDC through this project. Sadly, we were out of the game in the stage of school contest.\nThe radar works in the 2.7 GHz - 3.7 GHz band. It has 4 transmitting antenna and 12 receive antenna. With a transmission power of less than 10 dBm, it localizes a person within a median of 5 cm in the y dimension. Within 5 meters, it has an accuracy of about 20 cm. Without further tests, the maximum distance is unknown. However, it can indicate a person behind a 25 cm concrete wall.\nCode and document here. The final presentation with hardware and data processing flow introduction see here.\nFMCW  Frequency-modulated continuous-wave radar (FM-CW) – also called continuous-wave frequency-modulated (CWFM) radar – is a short-range measuring radar set capable of determining distance (Continuous-wave radar wiki).\n If you have no idea about it, the tutorial below is a good starting point.\n Frequency-Modulated Continuous-Wave Radar (FMCW Radar): a brief introduction of the principle of FMCW Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging: an MIT course to build an FMCW radar yourself GUEST POST: TRY RADAR FOR YOUR NEXT PROJECT: an article written by the instructor of the course above Intro to mmWave Sensing: FMCW Radars - Module 1: Range Estimation: a series of 5 short videos providing a concise yet in-depth introduction to sensing using FMCW radars  SDR  Software-defined radio (SDR) is a radio communication system where components that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system.\n The project uses a USRP N210 with an LFRX daughter board. SDR is not an important part of this project cause the RF front end are made with RF modules and the FMCW generator is build separately. The USRP here acts as a role of ADC and is accessed via Simulink. See how to use a USRP with Simulink below:\n USRP® Support Package from Communications System Toolbox: Matlab toolbox for USRP Digital Communication Systems Engineering Using Software Defined Radio: a tutorial of SDR using Simulink   Hardware The hardware design can be introduced in two parts, RF front end and FMCW generator. The design references the projects and the papers below:\n Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging RF-Capture: paper see RF-Capture: Capturing a Coarse Human Figure Through a Wall WiTrack: papers see 3D Tracking via Body Radio Reflections, Multi-Person Localization via RF Body Reflections  FMCW generator  ADF4159 evaluation board: EV-ADF4159EB3Z Loop filter: AD8065. It is designed with ADIsimPLL under the guidance of CN-0302. VCO: ZX95-3800A+ Power splitter: ZX10-2-42+ Attenuator: VAT-3+  The final design works in 2.7 GHz to 3.7 GHz to avoid 2.4 GHz band.\n  FMCW generator  RF front end  PA and LNA: ZX60-53LNB+ Power splitter: ZN2PD-9G+ Attenuator: FW-9+ Mixer: ZX05-43+ Switch: ADRF5040 mbed board: ST NUCLEO-L476RG  The mbed board detects the trigger edge and overlay current antenna pair number after the edge. The baseband signal from the mixer and coded sync signal from mbed board are sent into the same LFRX board to achieve synchronization, then will be analyzed in Simulink in real time.\n  RF front end   Software Software part includes ADF4159 evaluation board configuration, mbed code, and Simulink model.\nADF4159 The ADF4159 evaluation board is configured using ADF4158 and ADF4159 Evaluation Board Software via USB. The configuration file here can be loaded into the software. The board is configured as follows:\n Ramp mode: Continuous sawtooth RF frequency: 2.7 GHz - 3.7 GHz Ramp frequency: 2000 Hz Charge pump: 1.25 mA Muxout: Digital lock Detect. It is used as a sync signal to trigger the switching of antennas.  The 4 Tx and 12 Rx makes up 4*12 antenna pairs. With ramp frequency of 2000 Hz, every antenna pair is switched to for 41.7 times per second.\nmbed code The mbed board switches antenna pairs after every trigger edge from the ADF4159 Muxout pin. And it also overlay the antenna pair number to the sync signal. Considering that MCU has some unstable reaction delay between the trigger edge of the sync signal and the first down edge of the output, the MCU first pulls down the sync signal then output bits of the number. In this way, the down edge of the overlaid signal has no jitter. Code of mbed board is here.\n  Antenna switching timing diagram  Simulink model All the models are in the simulink folder. RadarImagingAndPositioning.slx block implements the basic signal processing logic and output the 2D heat map and target position. The data processing flow is shown in the presentation above. usrp_4t12r_heatmap.slx connects the basic blocks to show the imaging result and targets. For more information about using USRP with Simulink, see the Introduction section above.\n Results In the experimental stage, I built a system with 1 Tx and 3 Rx.\n  1TX3RX system  The overlaid sync signal is shown below.\n  Sync signal  1D imaging of each antenna pair is shown below. Tested with a person walking away then walking back. The d axis shows the round trip of the radar signal.\n  1d imaging of each antenna pair  The system was then expanded to 1 Tx and 8 Rx. In this stage, 2D imaging can be done.\n  1TX8RX system  The following image is the heat map captured from the 1Tx8Rx system. The x-axis indicates angle from left to right. y-axis indicates the round distance of the radar signal. This illustration was made in a hurry so there is no complete annotation.\n  1Tx8Rx top-down heatmap  After that, the system was expanded to 4 Tx and 12 Rx. In this stage, 3D imaging can be done.\n  4TX12RX system  The following gif shows the 3D imaging projected to xz-plane of a person holding a corner reflector and drawing a circle in the air.\n  Heatmap front view  The gif below shows the 2D imaging of xy-plane with target tracking. The target person was walking around in the distance of 3 meters. The y-axis indicates the round distance of radar signal.\n  Heatmap top-down view  I tried to extract the z-axis coordinate of person target. However, because of the reflection angle mentioned in RF-Capture, and the intensified effect by the lower frequency band of our system, the height of a person cannot be accurately detected.\nThe image shows the heat map of the target in the z-axis changing with time. The target is walking in place then squats in the 10th second. As shown below, the height drops several times when the target moves.\n  Heatmap z  The final work was put on a cart.\n  Final system  ","date":1527465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527465600,"objectID":"c6fa4c6a2d34bff5f689074f080f31dd","permalink":"https://chengzhag.github.io/project/fmcw_positioning_radar/","publishdate":"2018-05-28T00:00:00Z","relpermalink":"/project/fmcw_positioning_radar/","section":"project","summary":"A FMCW Radar with 4 transmitting antennas and 12 receive antennas working within the 2.7 GHz - 3.7 GHz band that can locate a person in the same room with accuracy of 20 cm.","tags":["Radar","Contest","Electronics Design"],"title":"FMCW Positioning Radar","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Big Plate project is a work of team made up with three people. We are fortunate enough to get the first prize with this in 2017NUEDC.\nI was responsible for building the structure and programming STM32 and Raspberry Pi. Code and document here. The other two teammates are shicaiwei123 and zianglei. Sadly that yoyolalala left the team before the contest. The preparation projects below can not be done without her. I\u0026rsquo;d like to thank her here.\nThe final work has been handed in to our school and there will be no plan to write a detailed document for it. This page tells the story behind it.\n The ball-on-plate system is a promoted version of the traditional ball-on-beam control problem. The problem consists of a plate which its deviation can be manipulated in two perpendicular directions. The goal is to carry the ball moving on the plate to a desired position, that is to control a freely rolling ball on a specific position or moving on a trajectory on the plate. - Modelling and Control of Ball-Plate System   Ball-on-plate system \n Specifically in this project, according to the contest requirements, the system is supposed to move a ball between any two of nine evenly distributed circles drawn on the plate and avoids other circles. The plate must have a size around 65cm*65cm. The document shows the distribution of the circles.\n The Contest NUEDC is one of the largest electronic design competitions in China. The competition last three and a half days. Four categories of more than ten subjects are published on the official website on the day it begins. Participants usually prepare for a long time before the competition. The competition in 2017 which I attended started from August 9 to August 12. Our team started preparing about four months in advance.\nI was responsible for building the structure and programming STM32 and Raspberry Pi. Although I had got some experience with programming, I barely knew the basics of control system. However, I chose the control system direction to prepare because of my interest in it.\nIn the preparation process, we learned from five projects, three of which are listed below.\n Rotary Inverted Pendulum Ball on Beam Ball on Small Plate  Sadly, I\u0026rsquo;ve got no video demo of the final work since we were too busy moving forward. So let\u0026rsquo;s jump into the details of the design.\n Platform  Raspberry Pi Zero and Raspberry Pi 3b with OpenCV installed STM32F103 minimum system board PC with Visual Studio and VisualGDB installed  The Raspberry Pi is developed with C++ language using Raspberry toolchain provided by VisualGDB (tutorial here). The STM32 is developed with C++ language using Arm toolchain provided by VisualGDB (tutorial here). The Raspberry runs OpenCV program and sends the results to STM32 through UART.\nI made use of a C++ API written for STM32 named ebox. Although it saved me a lot of time then, I found mbed even easier than it after the competition. By the way, it was said that Arduino is not allowed in the competition.\n Mechanical structure The final work uses a PCB motherboard made in Ball on Small Plate project to put the electronic modules together.\n  PCB mother board  The 65cm*65cm plate is made of light wood laminate board.\n  The plate  There are ribs below the plate to support it. The dynamic structure of the plate is same as Ball on small plate.\n  Under the plate  Design documents of the reinforcing ribs and others are also on the GitHub page.\n  Reinforcing ribs    Cardan joint    Servo with connecting rod  The camera is supported by carbon fiber tubes like in Ball on small plate. And the same LED circle is installed around the camera.\n  LED around camera  The Raspberry Pi 3b is hanged beyond the camera to reduce the load on the servos.\n  The complete work  After the work was done and tested by ourselves, we packed it with cardboard then handed it over to the competition organizer.\n  Complete work packed with cardboard, kinda like a house   Final test After a successful test in front of the judges of Sichuan division and a small test on our electronic design capabilities, we stepped into our journey to Xi\u0026rsquo;an. Here show some photos taken along the trip.\n      ","date":1504396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504396800,"objectID":"fedfb20833290a8d7531050fb8d70ca1","permalink":"https://chengzhag.github.io/project/ball_on_big_plate/","publishdate":"2017-09-03T00:00:00Z","relpermalink":"/project/ball_on_big_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 65cm wide plate, and is a work of 3 team-mates which get us first prize in 2017NUEDC.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Big Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Small Plate is built with a Raspberry Zero, a camera, two servos, a STM32F103 board. Capable of setting target coordinate, circle around the center with adjustable radius.\nBall on Small Plate project is a work of team. The other two teammates are shicaiwei123 and yoyolalala.\nCode and document here. Platform details here.\n Demo Primary demo with very limited precision and robustness is shown below.\n   Mechanical structure First version The 20cm*20cm plate is made of 1mm thick black glass fiber board. In the first version shown in the video, the plate is covered with white paper to provide contrast between ball and plate. Below shows the original plate.\n  Deisigning the mechanical structure  The plate is supported by a universal joint with 2 degrees of freedom. Two servos control the angle of the plate through ball bearing connecting rods.\n  Installation of plate and two servos  In this first version. The camera and a Raspberry Pi Zero is beyond the plate and fixed to the base. In this case, the area of the plate must be segmented and projected into a square. The noise introduced in this progress is one of the reasons for the jitter in the video.\n  Installation of camera and Raspberry Pi Zero  Second version I managed to reduce the jitter by fixing the camera with carbon fiber tubes to the plate in the second version. The plate itself is also changed to yellow. A Raspberry Pi 3b is hanged beyond the camera.\n  Camera is connected to the plate to reduce jitter  A LED circle is around the camera for illustration.\n  Camera and LED installation  Below shows the PCB board also used in the later project Ball on Big Plate.\n  PCB mother board  ","date":1496534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496534400,"objectID":"a849425d11bc301c05720583165b2c0d","permalink":"https://chengzhag.github.io/project/ball_on_small_plate/","publishdate":"2017-06-04T00:00:00Z","relpermalink":"/project/ball_on_small_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 20cm wide plate, and is capable of setting target coordinate, circle around the center with adjustable radius.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Small Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Beam is built with a Raspberry Zero, a camera, one servo, a STM32F103 board. Ball on Beam project is a work of team. The other two teammates are shicaiwei123 and yoyolalala. Code and document here. Platform details here.\n Demo Primary demo with very limited precision and robustness is shown below.\n  ","date":1495929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495929600,"objectID":"052b23df82dcd5f3f66e60e43c26940f","permalink":"https://chengzhag.github.io/project/ball_on_beam/","publishdate":"2017-05-28T00:00:00Z","relpermalink":"/project/ball_on_beam/","section":"project","summary":"Balancing a ball on a beam with a Raspberry Zero, a camera, one servo, a STM32F103 board.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Beam","type":"project"},{"authors":null,"categories":null,"content":" Introduction Rotary Inverted Pendulum is built with an encoder, a brush motor, a STM32F103 board. No rotation limit. Capable of setting target angle, non-stop swinging and automatically swinging up.\nRotary Inverted Pendulum project is a work of team. The other two teammates are shicaiwei123 and yoyolalala.\nCode and document here. Platform details here.\n Demo Primary demo with very limited precision and robustness is shown below.\n   Mechanical structure I use an optical encoder to measure the angle of the pendulum. The absolute angle of the pendulum need calibrate every time after reset.\n  Optical encoder  A slip ring is used to prevent the winding of the wire.\n  Slip ring  The DC brush motor with gearbox and magnetic encoder is driven by a TB6612FNG model controlled with PWM signal. A 3s lithium battery powers the system like the following projects.\n  Complete circuit board  The mechanical structure includes three round mounting plates to mount slip ring, motor, and quick-release plate. The plates are connected with hexagonal copper pillars. The whole structure is mounted to a tripod through the quick-release plate.\nThe round plates are rendered as acrylic plate below. But they are made of carbon fiber in the final work.\n  The core mechanical structure    The complete mechanical structure   Software design  Please refer to Study on PID Control of a Single Inverted Pendulum System for the PID control system.   PID control system \n  Also we refer to Research and Implementation of the Swing - up and Stabilizing Operation for Rotational Inverted Pendulum for the swing-up and stabilizing operation.   Swing - up and stabilizing operation \n The swing-up and stabilizing operation and the changes between different modes are modeled as a state machine.\n  State machine for swing-up and stabilizing operation  ","date":1491696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491696000,"objectID":"a8f9d12f6d2d712d537277742928f936","permalink":"https://chengzhag.github.io/project/rotary_inverted_pendulum/","publishdate":"2017-04-09T00:00:00Z","relpermalink":"/project/rotary_inverted_pendulum/","section":"project","summary":"A rotary inverted pendulum system built with an encoder, a brush motor, a STM32F103 board that is capable of setting target orientation, non-stop swinging and automatically swinging up.","tags":["Contest","Control System","Electronics Design"],"title":"Rotary Inverted Pendulum","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://chengzhag.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]