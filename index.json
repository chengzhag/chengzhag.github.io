[{"authors":null,"categories":null,"content":"I am a second-year master‘s student of machine learning and computer vision under the supervision of Prof. Shuaicheng LIU at University of Electronic Science and Technology of China. During my master study, I worked closely with Prof. Zhaopeng Cui and Dr. Yinda Zhang. My research interests include 3D reconstruction, 3D detection, 3D scene understanding, etc. I received my Bachelor\u0026rsquo;s degree at UESTC in 2019.\n","date":1615420800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615420800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a second-year master‘s student of machine learning and computer vision under the supervision of Prof. Shuaicheng LIU at University of Electronic Science and Technology of China. During my master study, I worked closely with Prof.","tags":null,"title":"Cheng Zhang","type":"authors"},{"authors":["Cheng Zhang","Zhaopeng Cui","Yinda Zhang","Bing Zeng","Marc Pollefeys","Shuaicheng Liu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- CVPR 2021  Cheng Zhang 2 \u0026nbsp; \u0026nbsp; Zhaopeng Cui 1 \u0026nbsp; \u0026nbsp; Yinda Zhang 3 \u0026nbsp; \u0026nbsp; Bing Zeng 2 \u0026nbsp; \u0026nbsp; Marc Pollefeys 4 \u0026nbsp; \u0026nbsp; Shuaicheng Liu 2   1 State Key Lab of CAD \u0026 CG, Zhejiang University 2 University of Electronic Science and Technology of China \u0026nbsp; \u0026nbsp; 3 Google \u0026nbsp; \u0026nbsp; 4 ETH Zurich     Abstract We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shape, object pose, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.\n Paper          arXiv   -- Paper -- Cite  \u0026nbsp; \u0026nbsp; arXiv  \u0026nbsp; \u0026nbsp; Paper  \u0026nbsp; \u0026nbsp; Code   [arXiv] \u0026nbsp; \u0026nbsp; [Paper] \u0026nbsp; \u0026nbsp; [Supp] \u0026nbsp; \u0026nbsp; [GitHub]  --  Motivations  Implicit representation like Signed Distance Function (SDF) can be used to detect collision and propagate gradients And together with structured representation (LDIF), the shapes can be learned better and more shape priors can be provided for relationship understanding Graph Convolutional Network (GCN) is proven to be good at resolving context information in the task of scene graph generation   Pipeline   Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.  The proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage. In the initial estimation stage, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry. The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose. In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.\n Results   Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.  ","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"83623508ea0f4df7d9acb4b00885826b","permalink":"https://chengzhag.github.io/publication/im3d/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/im3d/","section":"publication","summary":"We present a new pipeline which takes a single image as input, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation.","tags":["Deep Learning","3D Reconstruction","3D Detection","3D Scene Understanding","Layout Estimation"],"title":"Holistic 3D Scene Understanding from a Single Image with Implicit Representation","type":"publication"},{"authors":null,"categories":null,"content":" Introduction For a small research team with more than a dozen of students, several servers with 4 or 8 GPUs are necessary. However, with independent storage for each servers, datasets, codes, and environments are often duplicated across different servers, which is inefficient and troublesome when using different servers. Also, user management becomes a problem if every user needs to be setup on each of the server.\nTo solve the above problems, a system is built with centralized network storage, server and user management, 10 GbE network, and per-server SSD cache. This can be used as a reference for small academic teams on deep learning or maybe small companies.\nFor new users and admins, documents (in Chinese) are wrote to provide basic informations and regulations.\nHardware Servers and Network Our team has 5 existing servers, each with 4 x 1080Ti and a system SSD. The problem is that every server is not able to install an extra 10 GbE Network Interface Card (NIC) since all PCIE ports are preserved for GPUs. Thus an NBASE-T switch is used to connect each server via 5 GbE USB NICs. (A more stable and affordable configuration might be using 10 GbE SFP+ switch with PCIE NIC.)\n  No space to install 10 GbE NIC for existing server  Storage We chose to use QNAP Network Attached Storage (NAS) TS-932X, which can accommodate 5 x 3.5-inch hard drives and 4 x 2.5-inch SSDs, and has dual 10GbE SFP+ ports. The performance of its ARM processor is quit as SSD cache is configured on each server, which makes it possible to load frequently used data from NAS at the first time.\n  NAS and switch  Software NAS We configure our NAS with a Public folder (contraining the git repo of documents) for shared datasets storage, and a home folder for each user, allowing users to access their environment (e.g. anaconda), codes, and datasets on every server. QNAP NAS provides an easy-to-use operating system with serveral customizable services. In our cases, it is configured with the following services:\n NFS server: For mounting storage on server samba server: For mounting storage on client computers LDAP server: Centralized authorization and user management for both NAS and GPU server Other services like Download Station, file management from browser, restoring from snapshot/recycle bin, Qsync, VPN, DDNS, and iperf3 are also available to users.  Server Each server is installed with same version of Ubuntu and Nvidia GPU driver, and is configured with the following services:\n autofs: Automatically mount two types of folders (i.e. user home folders to /home and Public folder to /media) through NFS cachefilesd: Local SSD cache for NFS LDAP authentication: Configured with Pluggable Authentication Modules (PAM) ThinLinc: Remote desktop  With the configurations above, students can use the same username to login to NAS or any server, access their data or share datasets with others, and configure anaconda environments once to use them on all servers. A shell script is used to configure servers. It is validated on Ubuntu 18 and might be used as a reference. For easier deployment and management of servers, virtual machine system (e.g. Proxmox) is installed on each server, with GPU passthrough configured. The difference between virtual machine and bare-metal Ubuntu is barely sensible.\n","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"b9bcc7ff90adaaa3fbb8f29daff8aa1e","permalink":"https://chengzhag.github.io/project/deep_earning_server_system/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/project/deep_earning_server_system/","section":"project","summary":"A deep learning system for small research teams, with multiple GPU servers, centralized network storage, server and user management, 10 GbE network, and per-server SSD cache.","tags":["Deep Learning"],"title":"Deep Learning Server System","type":"project"},{"authors":null,"categories":["Photoblog"],"content":"The photo is taken from an airplane when preparing to land in Chengdu. The conditions and timing for taking this photo were demanding. First, the city lights in the backgound is only visible in a full left turn. Second, measures must be taken to avoid the shaking of hand and the reflection on the window.\n  Nikon D3300. f/5.6, 20mm, 5s, ISO 1600.  ","date":1566000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566000000,"objectID":"ebc4af732736ca6c7dfe5aa2c6426470","permalink":"https://chengzhag.github.io/post/plane_turn_long_exposure/","publishdate":"2019-08-17T00:00:00Z","relpermalink":"/post/plane_turn_long_exposure/","section":"post","summary":"The conditions and timing for taking this photo were demanding. First, the city lights in the backgound is only visible in a full left turn. Second, measures must be taken to avoid the shaking of hand and the reflection on the window.","tags":["Photo"],"title":"Long Exposure of Airplane Turning","type":"post"},{"authors":null,"categories":null,"content":" Introduction This project is a work of team of 3, which includes hsuhaoo and Zijing Guan. It is developed for the contest of ESDC. But we did not managed to realize the full ideas of fall detection for the elderly.\nThe radar works in the 2.7 GHz - 3.7 GHz band, and has 4 transmitting antennas and 12 receive antennas. With the power of less than 10 dBm, it can measure the distance of a person with a median of 5 cm. Within 5 meters, it has an positioning accuracy of about 20 cm. Without further tests, the maximum distance is unknown. However, it can indicate the existance of a person behind a 25 cm concrete wall.\nCode and documents are available here. Slides introducing hardware and data processing process (in Chinese) are available here.\nFMCW  Frequency-modulated continuous-wave radar (FM-CW) – also called continuous-wave frequency-modulated (CWFM) radar – is a short-range measuring radar set capable of determining distance (Continuous-wave radar wiki).\n If you have no idea about FMCW, the following tutorials can be a good starting point.\n Frequency-Modulated Continuous-Wave Radar (FMCW Radar): a brief introduction of the principle of FMCW Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging: an MIT course to build an FMCW radar yourself GUEST POST: TRY RADAR FOR YOUR NEXT PROJECT: an article written by the instructor of the course above Intro to mmWave Sensing: FMCW Radars - Module 1: Range Estimation: a series of 5 short videos providing a concise yet in-depth introduction to sensing using FMCW radars  SDR  Software-defined radio (SDR) is a radio communication system where components that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system.\n We use a USRP N210 with an LFRX daughter board to sample the baseband signal. The RF front end is made with RF modules and the FMCW generator is build on a VCO. The USRP here acts as an ADC and is connected to Simulink. Following links show how to connect a USRP to Simulink:\n USRP® Support Package from Communications System Toolbox: Matlab toolbox for USRP Digital Communication Systems Engineering Using Software Defined Radio: a tutorial of SDR using Simulink   Hardware In this section, we introduce the hardware design in two parts, i.e. RF front end and FMCW generator. We refer to the following projects and papers for basic ideas:\n Build a Small Radar System Capable of Sensing Range, Doppler, and Synthetic Aperture Radar Imaging RF-Capture: paper see RF-Capture: Capturing a Coarse Human Figure Through a Wall WiTrack: papers see 3D Tracking via Body Radio Reflections, Multi-Person Localization via RF Body Reflections  FMCW generator Hardware:\n ADF4159 evaluation board: EV-ADF4159EB3Z Loop filter: AD8065. It is designed with ADIsimPLL under the guidance of CN-0302. VCO: ZX95-3800A+ Power splitter: ZX10-2-42+ Attenuator: VAT-3+  Our FMCW generator works in 2.7GHz-3.7GHz band to avoid 2.4GHz wifi signal.\n  FMCW generator  RF front end Hardware:\n PA and LNA: ZX60-53LNB+ Power splitter: ZN2PD-9G+ Attenuator: FW-9+ Mixer: ZX05-43+ Switch: ADRF5040 mbed board: ST NUCLEO-L476RG  The mbed board detects the trigger edge and add a digital signal after the edge to indicate id of antenna pair. The baseband signal from the mixer and the sync signal from mbed board are first sampled with the LFRX daughter board simultaneously, then analyzed with Simulink in real time.\n  RF front end   Software In this section, we introduce software design in three parts, i.e. ADF4159 evaluation board configuration, mbed code, and Simulink model.\nADF4159 The ADF4159 evaluation board is configured using ADF4158 and ADF4159 Evaluation Board Software via USB. The configuration file is available here. The board is configured with follow parameters:\n Ramp mode: Continuous sawtooth RF frequency: 2.7 GHz - 3.7 GHz Ramp frequency: 2000 Hz Charge pump: 1.25 mA Muxout: Digital lock Detection. It is used as a trigger to switch antenna pairs and a sync signal to align base band signals of each antenna pair. The muxout port outputs a falling edge on the start of a ramp.  The 4 Tx and 12 Rx antennas makes up 4*12 antenna pairs. With ramp frequency of 2000 Hz, every antenna pair can be activated for 41.7 times per second.\nmbed code The mbed board switches antenna pairs after every trigger edge from the ADF4159 Muxout pin. And as mentioned before, it adds a digital signal after the edge to indicate id of antenna pair. Considering that MCU is running at a relatively low frequency, which makes the responce to the sync signal unstable, we design the digital signal from MCU to pull down the sync signal after the trigger edge. In this way, the falling edge is kept synced. Code of mbed board is available here.\n  Antenna switching timing diagram  Simulink model All the models are in the simulink folder. RadarImagingAndPositioning.slx block process baseband signals from USRP and output the 2D heat map and target position. The data processing pipeline is shown in the slides mentioned before. usrp_4t12r_heatmap.slx cooperates the above block with others to show the imaging and target detection results.\n Results In the experimental stage, A system with only 1 Tx and 3 Rx is built for experimental usage.\n  1TX3RX system  The sync signal is shown below.\n  Sync signal  Waterfall plot of each antenna pair is shown below. A person is walking away then back in this experiment. The d axis shows the round trip of the radar signal.\n  waterfall plot of each antenna pair  The system is then expanded to 1 Tx and 8 Rx, which allows 2D imaging with phased array algorithm.\n  1TX8RX system  The image bellow shows the heat map captured from the 1Tx8Rx system. The x-axis indicates angle from left to right. The y-axis indicates the round distance of the radar signal. A person is standing in front of the radar at a distance of about 4 meters in this case.\n  1Tx8Rx top-down heatmap  The system is further expanded to 4 Tx and 12 Rx, which allows 3D imaging.\n  4TX12RX system  The following GIF is the 3D imaging projected to the front view, showing a person drawing a circle in the air with a corner reflector.\n Front view heatmap  The GIF below shows the 2D imaging from above with target detection activated. A person is walking around in the distance of about 3 meters. The y-axis indicates the round distance of radar signal.\n Top-down view heatmap  We tried to extract the signal strength vertical axis of person target doing squatting in the 10th second. However, as the radio waves in this frequency acts more like mirror reflection on the human body, the angle of the reflection surface becomes a issue as mentioned in RF-Capture. This makes the height of a person unpredictable for our system.\nThe figure bellow shows the strength of the reflected signal in the vertical axis vs time. The heat map indicates that the height of the person drops several times during the first 10 seconds, while the target only moves slightly with no changes on height.\n  Height axis heatmap vs time  The final work was put together on a cart.\n  Final system  ","date":1527465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527465600,"objectID":"c6fa4c6a2d34bff5f689074f080f31dd","permalink":"https://chengzhag.github.io/project/fmcw_positioning_radar/","publishdate":"2018-05-28T00:00:00Z","relpermalink":"/project/fmcw_positioning_radar/","section":"project","summary":"A FMCW Radar with 4 transmitting antennas and 12 receive antennas working within the 2.7 GHz - 3.7 GHz band that can locate a person in the same room with accuracy of 20 cm.","tags":["Radar","Contest","Electronics Design"],"title":"FMCW Positioning Radar","type":"project"},{"authors":null,"categories":null,"content":" Introduction This project is a work of team of 3, which includes shicaiwei123 and zianglei. The preparations of the contest are done with the help of yoyolalala. I was responsible for building mechanical structures and embedded programming (i.e. STM32, Raspberry Pi). We are lucky to have received the first prize in 2017NUEDC. The final work has been handed in to our school thus details will not be provided. However, code and document can be found here.\n The ball-on-plate system is a promoted version of the traditional ball-on-beam control problem. The problem consists of a plate which its deviation can be manipulated in two perpendicular directions. The goal is to carry the ball moving on the plate to a desired position, that is to control a freely rolling ball on a specific position or moving on a trajectory on the plate. - Modelling and Control of Ball-Plate System   Ball-on-plate system \n Specifically in this project, according to the contest requirements, the system is supposed to move a ball between any two of nine evenly distributed circles and avoids the others. The plate must have a size around 65cm*65cm. The document shows the distribution of the circles.\n The Contest NUEDC is one of the largest electronic design contest for undergraduate students in China. The contest lasts three and a half days. More than ten topics of four types including control, measurement, communication and power are published in the first day morning, indicating the start of the contest. Participants usually prepare for a long time before the contest. We attended the contest in 2017 starting from August 9th to 12th. Our team started the preparation four months in advance.\nAlthough I had got some experience with programming, I barely knew the basics of control system. However, I chose this type of topic just in the interest of it.\nIn the preparation process, we learned from five projects, three of which are listed below.\n Rotary Inverted Pendulum Ball on Beam Ball on Small Plate  Sadly, no video demo was taken for the limit of time.\n Platform  Raspberry Pi Zero and Raspberry Pi 3b with OpenCV installed STM32F103 minimum system board PC with Visual Studio and VisualGDB installed  The Raspberry Pi is developed with C++ using Raspberry Pi toolchain provided by VisualGDB (tutorial here). It deals with the machine vision tasks and sends the results to STM32 through UART.\nThe STM32 is developed with C++ language using Arm toolchain provided by VisualGDB (tutorial here). A C++ API for STM32 (Ebox) was used as a replacement of ardunio, which is not allowed in the contest.\n Mechanical structure The final work uses a PCB motherboard (designed by shicaiwei123) same as Ball on Small Plate project to connect modular electronic parts.\n  PCB mother board  The 65cm*65cm plate is made of laminated balsa wood board.\n  The plate  We design metre-shaped ribs to further reinforce the plate. We follow Ball on small plate to support the plate with a universal joint in the center and drive the board with two servos using ball joint rods as connection.\n  Under the plate  Design documents of the reinforcing ribs are also available here.\n  Reinforcing ribs    Cardan joint    Servo with connecting rod  The camera is supported by carbon fiber tubes like in Ball on small plate. And the same ring light is installed around the camera to provide better illumination.\n  LED around camera  The Raspberry Pi 3b is hanged beyond the camera to reduce the load on the servos.\n  The complete work  The final work is packed with cardboard then submitted to the competition organizer.\n  Complete work packed with cardboard, kinda like a house   Final test After a successful test in front of the judges of Sichuan division, follwed by a basic on-site electronic design test, we went to Xi\u0026rsquo;an for the final test. Below shows some photos taken along the trip.\n      ","date":1504396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504396800,"objectID":"fedfb20833290a8d7531050fb8d70ca1","permalink":"https://chengzhag.github.io/project/ball_on_big_plate/","publishdate":"2017-09-03T00:00:00Z","relpermalink":"/project/ball_on_big_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 65cm wide plate, and is a work of 3 team-mates which get us first prize in 2017NUEDC.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Big Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Small Plate is built with a Raspberry Zero, a camera, two servos, a STM32F103 board, and is capable of setting target coordinate, circling around the center with adjustable radius. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details are discussed here.\n Demo    Mechanical structure First version The 20cm*20cm plate is made of 1mm black glass fiber board. In the first version shown in the video, the plate is covered with white paper to provide more contrast between ball and plate. Below shows the original plate.\n  Deisigning the mechanical structure  The plate is supported with a universal joint in the center and has 2 degrees of freedom. The angle of the plate is controled by two servos with ball joint rods connecting the servos and the plate.\n  Installation of plate and two servos  In this first version. The camera is fixed to the base, which requires extra procedure to segment the plate from background and warp back to squre. This introduces noise to the positioning of the ball, which is the major reason for the jittering in the video.\n  Installation of camera and Raspberry Pi Zero  Second version By fixing the camera to the plate with carbon fiber tubes, we managed to avoid the noise. The Raspberry Pi 3b is hanged beyond the camera to reduce the load on the servos. The plate itself is also changed to yellow thus no paper is needed. This reduces friction between the ball and the plate.\n  Camera is connected to the plate to reduce jitter  A ring light is installed around the camera for better illumination.\n  Camera and LED installation  Below shows the PCB board installed on the base, which is later used in project Ball on Big Plate.\n  PCB mother board  ","date":1496534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496534400,"objectID":"a849425d11bc301c05720583165b2c0d","permalink":"https://chengzhag.github.io/project/ball_on_small_plate/","publishdate":"2017-06-04T00:00:00Z","relpermalink":"/project/ball_on_small_plate/","section":"project","summary":"The project focus on balancing and controlling the movement of a metal ball on a 20cm wide plate, and is capable of setting target coordinate, circle around the center with adjustable radius.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Small Plate","type":"project"},{"authors":null,"categories":null,"content":" Introduction Ball on Beam is built with a Raspberry Zero, a camera, a servo, and a STM32F103 board. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details are discussed here.\n Demo   ","date":1495929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495929600,"objectID":"052b23df82dcd5f3f66e60e43c26940f","permalink":"https://chengzhag.github.io/project/ball_on_beam/","publishdate":"2017-05-28T00:00:00Z","relpermalink":"/project/ball_on_beam/","section":"project","summary":"Balancing a ball on a beam with a Raspberry Zero, a camera, one servo, a STM32F103 board.","tags":["Contest","Control System","Electronics Design"],"title":"Ball on Beam","type":"project"},{"authors":null,"categories":null,"content":" Introduction Rotary Inverted Pendulum is built with an encoder, a brush motor, a STM32F103 board, and has no rotation limit. It is capable of setting target angle, non-stop swinging and automatically swinging up. This project is a work of team of 3, which includes shicaiwei123 and yoyolalala. Code and documents are available here. Platform details Code and documents are available here.\n Demo    Mechanical structure An optical encoder is used to measure the angle of the pendulum. Thus calibration needs to be down by relaxing the pendulum to get the absolute angle every time after reset.\n  Optical encoder  A slip ring (or collector ring) is used to prevent the winding of the wire.\n  Slip ring  The DC brush motor with reduction gearbox and magnetic encoder is driven by a TB6612FNG model controlled controled with PWM signal. A 3-ceils lithium battery is used as power.\n  Complete circuit board  The mechanical structure consists of three round plates which supports the slip ring, motor, and a quick-release plate. The round plates are connected with hexagonal copper pillars. The whole structure is mounted to a tripod through the quick-release plate.\n  The core mechanical structure    The complete mechanical structure   Software design  Please refer to Study on PID Control of a Single Inverted Pendulum System for the PID control system.   PID control system \n  Also we refer to Research and Implementation of the Swing - up and Stabilizing Operation for Rotational Inverted Pendulum for the swing-up and stabilizing operation.   Swing - up and stabilizing operation \n The swing-up and stabilizing operation and the changes between different modes are modeled as a state machine.\n  State machine for swing-up and stabilizing operation  ","date":1491696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491696000,"objectID":"a8f9d12f6d2d712d537277742928f936","permalink":"https://chengzhag.github.io/project/rotary_inverted_pendulum/","publishdate":"2017-04-09T00:00:00Z","relpermalink":"/project/rotary_inverted_pendulum/","section":"project","summary":"A rotary inverted pendulum system built with an encoder, a brush motor, a STM32F103 board that is capable of setting target orientation, non-stop swinging and automatically swinging up.","tags":["Contest","Control System","Electronics Design"],"title":"Rotary Inverted Pendulum","type":"project"},{"authors":null,"categories":["Photoblog"],"content":"Both campuses of University of Electronic Science and Technology of China (UESTC) in Chengdu are famous for their ginkgo. When autumn is in full swing, there is a constant stream of visitors. The scenes are captured as polar panoramas to show the full view of which.\n  Shahe Campus: No.4, Section 2, North Jianshe Road, Chengdu, Sichuan, China    Qingshuihe Campus: No.2006, Xiyuan Ave, West Hi-Tech Zone, Chengdu, Sichuan, China  ","date":1448755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448755200,"objectID":"41a65b93cd5ea0439f91a36ff2733da4","permalink":"https://chengzhag.github.io/post/ginkgo_panorama/","publishdate":"2015-11-29T00:00:00Z","relpermalink":"/post/ginkgo_panorama/","section":"post","summary":"Both campuses of UESTC in Chengdu are famous for their ginkgo. When autumn is in full swing, there is a constant stream of visitors.","tags":["Photo"],"title":"Ginkgo of UESTC","type":"post"},{"authors":null,"categories":["Photoblog"],"content":"In Milky Way Photography, Japanese photography Masahiro Miyasaka created a method to capture close-range objects (e.g. flowers) and stars far away in a single shot. With an automatic aperture lens, I managed to do the same while keeping the gorgeous bokeh by changing the focus distance instead of aperture size during the exposure time. After changing the focus distance, the flowers were illuminated with phone screen.\n  Nikon D3300. f/3.5, 18mm, 30s, ISO 3200.  ","date":1436745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436745600,"objectID":"e1beed3c22fbcdd82c8d521aee163e25","permalink":"https://chengzhag.github.io/post/qinhai_lake_milky_way/","publishdate":"2015-07-13T00:00:00Z","relpermalink":"/post/qinhai_lake_milky_way/","section":"post","summary":"In Milky Way Photography, Japanese photography Masahiro Miyasaka created a method to capture close-range objects (e.g. flowers) and stars far away in a single shot.","tags":["Photo"],"title":"Milky Way at Qinhai Lake","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://chengzhag.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]